{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OneVsOthersChar3HiddenStates.ipynb","version":"0.3.2","provenance":[{"file_id":"1G1nOb23uAdWRgvQrQ5wGEI1naBvj4ESp","timestamp":1555150360373},{"file_id":"1MwJSWKyMObQnQscRIRa0YJn5urCEbkRR","timestamp":1555016634588},{"file_id":"1ZIUZC-sPimWy1RSqtNoNEG7XX_yqNi71","timestamp":1554971018806}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WETg5r8EhbVo","colab_type":"code","colab":{}},"cell_type":"code","source":["import json\n","import torch\n","import re\n","import numpy as np\n","import collections\n","\n","from torch import nn\n","from torch.optim import SGD, Adam\n","from google.colab import files, auth, drive\n","from urllib.request import urlopen\n","from typing import List, Dict, Callable\n","from collections import Counter\n","from os import path\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import classification_report\n","drive.mount('/content/gdrive')\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QH0UNqvjmh6C","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  DEFINES  ================================='''\n","\n","'''======= Paths ======='''\n","\n","TrainPath = \"gdrive/My Drive/Datasets/Emoji/demojized_train.txt\"\n","TestPath = \"gdrive/My Drive/Datasets/Emoji/demojized_dev.txt\"\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-2oXk7VZSA-4","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  EXTERNAL FUNCTIONS  ================================='''\n","\n","\n","def readfile(filepath: str) -> str:\n","    \"\"\"\n","    Reads file and returns its content as a string.\n","    \"\"\"\n","    #response = urlopen(url)\n","    #body = response.read().decode('utf-8')\n","    with open(filepath, \"r\") as f:\n","        body = f.read()\n","    \n","    return body.encode('ascii', 'ignore').decode(\"utf-8\")\n","\n","  \n","def find_all(a_str, sub):\n","    start = 0\n","    while True:\n","        start = a_str.find(sub, start)\n","        if start == -1: return\n","        yield start\n","        start += len(sub) \n","  \n","  \n","  \n","\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    #print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7EmcK3GrlcPj","colab_type":"code","colab":{}},"cell_type":"code","source":["'''================================= CUSTOM DATA-STRUCTURES  ================================='''\n","\n","\n","class OneVsOthersDict():\n","  \n","  def __init__(self,):\n","      self.to_number = collections.OrderedDict()\n","      self.to_language = collections.OrderedDict()\n","    \n","  def insert(self,word):    \n","    if word == 'others':\n","      self.to_number[word] = 0\n","      self.to_language[0] = word\n","    else:\n","      self.to_number[word] = 1\n","      self.to_language[1] = 'feelings'\n","        \n","        \n","  def printitems(self):\n","      for label,value in self.to_number.items():\n","        print(label + ' ' + str(value))\n","        \n","  def getindex(self,word):\n","    return self.to_number[word]\n","  \n","  def getlabel(self,label):\n","    return self.to_language[label]\n","  \n","  def getlabellist(self):\n","    label_list = []\n","    for label,value in self.to_language.items():\n","      label_list.append(value)\n","    \n","    return label_list\n","\n","\n","class TwoWayDict():\n","  \n","  def __init__(self,):\n","    self.to_number = collections.OrderedDict()\n","    self.to_language = collections.OrderedDict()\n","    \n","\n","    \n","  def insert(self,word):\n","    if not word in self.to_number:\n","      new_index = len(self.to_number)\n","      self.to_number[word] = new_index\n","      self.to_language[new_index] = word\n","    \n","    \n","  def getindex(self,word):\n","    return self.to_number[word]\n","  \n","  def getlabel(self,label):\n","    return self.to_language[label]\n","  \n","  def getlabellist(self):\n","    label_list = []\n","    for label,value in self.to_language.items():\n","      label_list.append(value)\n","    \n","    return label_list\n","  \n","class Vocabulary:\n","    \"\"\"\n","    Helper class that maps characters to unique indices and the other way around\n","    \"\"\"\n","    def __init__(self, text: str):\n","        #special character for padding shorter sequences in a mini-batches\n","        characters_set = set(\"Â©\") \n","        characters_set.update(text)\n","\n","        \n","        self.char_to_idx = {char:idx for (idx, char) \n","                            in enumerate(characters_set)}\n","        \n","       \n","        self.idx_to_char = {idx:char for (idx, char) \n","                            in enumerate(characters_set)}\n","   \n","    def size(self):\n","        return len(self.char_to_idx)\n","      \n","    def __str__(self):\n","        return str(self.char_to_idx)\n","  \n","  \n","def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n","    \"\"\"\n","    Convert a string to a Tensor with corresponding character indices\n","    e.g. \"We have\" -> [12, 6, 20, 13, 1, 25, 6] \n","    \"\"\"\n","    \n","    \n","    text_indices = [vocab.char_to_idx[c] for c in text]\n","    \n","    return torch.tensor(text_indices)  \n","\n","  \n","def tensor_to_text(x: torch.LongTensor, vocab: Vocabulary) -> str:\n","    \"\"\"\n","    Convert a Tensor of character indices to its string representation\n","    e.g. [12, 6, 20, 13, 1, 25, 6] -> \"We have\"\n","    \"\"\"\n","    return \"\".join(vocab.idx_to_char[idx.item()] for idx in x) \n","  \n","  \n","def get_vocabulary(trainPath : str):\n","    \n","    text = readfile(trainPath)\n","    \n","    vocab = Vocabulary(text)\n","    \n","    return vocab\n","\n","\n","    \n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pwMshf8hhnY3","colab_type":"code","outputId":"1d720d98-b4da-4ef9-cba6-1711a1772964","executionInfo":{"status":"ok","timestamp":1555152004496,"user_tz":-180,"elapsed":1791,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["'''=================================   GLOVE EMBEDDING  ================================='''\n","\n","'''====== Load Glove Model======'''\n","\n","def loadGloveModel(gloveFile):\n","    print(\"Loading Glove Model\")\n","    f = open(gloveFile,'r')\n","    model = {}\n","    for line in f:\n","        splitLine = line.split()\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print(\"Done.\",len(model),\" words loaded!\")\n","    return model\n","\n","  \n","'''====== Get Word Embeddings ======'''\n","\n","def get_value(word,gloveModel):\n","  answer = torch.zeros(100,dtype=torch.float64)  \n","  if word in gloveModel.keys():\n","    answer = answer + torch.tensor(gloveModel[word],dtype=torch.float64)  \n","  return answer\n","\n","\n","def get_tensor_form(phrase,gloveModel):  \n","  \n","  phrase_tensor = get_value(phrase[0],gloveModel)\n","  for i in range(1,len(phrase)):\n","    phrase_tensor += get_value(phrase[i], gloveModel)\n","\n","  return phrase_tensor  \n","  \n","  \n","  \n","'''====== Instantiate Glove Model ======'''\n","  \n","  \n","#glovePath = \"gdrive/My Drive/Datasets/Emoji/glove.twitter.27B.100d.txt\"\n","#gloveModel = loadGloveModel(glovePath)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'====== Instantiate Glove Model ======'"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"6Hgm8lUEyCgm","colab_type":"code","colab":{}},"cell_type":"code","source":["def cut_phrase(phrase, max_length):\n","\n","  \n","  phrase_bounds = list(find_all(phrase, '<end>'))\n","\n","  leftsentence = \"\"\n","  middlesentence = \"\"\n","  rightsentence = \"\"\n","        \n","  for i in range(phrase_bounds[0],phrase_bounds[1]):\n","    leftsentence = leftsentence + phrase[i]\n","  \n","  for i in range(phrase_bounds[1],phrase_bounds[2]):\n","    middlesentence = middlesentence + phrase[i]\n","\n","  for i in range(phrase_bounds[2],phrase_bounds[3]):\n","    rightsentence = rightsentence + phrase[i]\n","\n","  \n","  l1 = len(leftsentence)\n","  l2 = len(middlesentence)\n","  l3 = len(rightsentence)\n","  \n","  '''\n","  -8 deoarece de la ultima se taie fix <end> si vrem sa adaugam si un spatiu\n","  de asemenea se modifica cea din mijloc si la stanga si la dreapta\n","  '''\n","  \n","  while(l1+l2+l3 > max_length - 8):\n","    \n","    if(l1 >= l2 and l1 >= l3):\n","      leftsentence = leftsentence[:-1]\n","      l1 -= 1\n","    elif(l2 >= l1 and l2 >= l3):\n","      middlesentence = middlesentence[:-1]\n","      l2 -= 1\n","    else:\n","      rightsentence = rightsentence[:-1]\n","      l3 -= 1\n","    \n","  middlesentence = \" \" + middlesentence + \" \"\n","  rightsentence = rightsentence + \" <end>\"\n","  \n","  #print(leftsentence + middlesentence + rightsentence  )\n","  \n","  return leftsentence + middlesentence + rightsentence  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_MpcBPm5m69f","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================   DATA LOADING  ================================='''\n","def get_data(path : str,  label_dict: TwoWayDict):\n","  print(path)\n","  labelcounter = 0\n","  words = []\n","  labels = []\n","  with open(path) as file:\n","    cnt = 0 \n","    for line in file:\n","      if cnt > 0:\n","        label = line.split()[-1]\n","        \n","        labels.append(label)\n","        \n","        templine = line.split(' ')[1:-1]\n","        \n","        words.append(templine)\n","\n","        label_dict.insert(label)\n","\n","      cnt+=1\n","  \n","  return [words,labels]\n","  \n","  \n","  \n","def get_dataset_loader(words, labels, label_dict, vocabulary, max_length=95,max_batch_idx=100, shuffle=True):\n","  \n","  dataset_length = len(labels)\n","\n","  traintensor = torch.zeros(dataset_length,max_length + 3) # Cele 3 hidden state-uri pe care le cautam :)\n","  labeltensor = torch.zeros(dataset_length,1)\n","  \n","  padding_tensor = torch.tensor([vocabulary.char_to_idx[\"Â©\"]])\n","  \n","  for i in range(0, dataset_length):\n","\n","    \n","    phrase = words[i]\n","    currentlabel = labels[i]  \n","    \n","    phrase = ' '.join(phrase)\n","    \n","    if(len(phrase) > max_length):\n","      phrase = cut_phrase(phrase,max_length)\n","    \n","    hidden_state_positions = []\n","    \n","    \n","    hidden_state_positions = list(find_all(phrase, '<end>'))\n","    hidden_state_positions = [x+4 for x in hidden_state_positions]\n","    hidden_state_positions.pop(0) # delete the first <end>\n","    \n","    \n","    phrase = text_to_tensor(phrase,vocabulary)\n","    \n","    while(phrase.size()[0] < max_length):\n","      phrase = torch.cat((phrase,padding_tensor))\n","    \n","    currentlabel = label_dict.getindex(currentlabel)\n","\n","    \n","\n","    hidden_state_positions = torch.tensor(hidden_state_positions)\n","    #print(hidden_state_positions)\n","    \n","    phrase = torch.cat((phrase,hidden_state_positions))\n","    phrase = phrase.to(device)\n","    \n","    #print(phrase.size())\n","    \n","    newlabel = torch.zeros(1).to(device).long()\n","    newlabel[0]=currentlabel\n","    currentlabel = newlabel\n","\n","    traintensor[i] = phrase\n","    labeltensor[i] = currentlabel\n","    \n","  \n","  large_dataset = TensorDataset(torch.tensor(traintensor).to(device), torch.tensor(labeltensor).to(device))\n","  large_data_loader = DataLoader(large_dataset, batch_size=max_batch_idx, shuffle=True, drop_last=True)\n","  return large_data_loader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xGQ5LFrVD2E1","colab_type":"code","outputId":"4871268a-fa20-45a7-f0c1-a9b23ff8e533","executionInfo":{"status":"ok","timestamp":1555152020767,"user_tz":-180,"elapsed":18040,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"cell_type":"code","source":["label_dict = OneVsOthersDict()\n","vocab = get_vocabulary(TrainPath)\n","\n","[trainingwords,traininglabels] = get_data(TrainPath,label_dict)\n","data_loader_train = get_dataset_loader(trainingwords,traininglabels,label_dict, vocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['batch_size'])\n","\n","[testingwords,testinglabels] = get_data(TestPath,label_dict)\n","data_loader_test  = get_dataset_loader(testingwords,testinglabels,label_dict, vocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['batch_size'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["gdrive/My Drive/Datasets/Emoji/demojized_train.txt\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["gdrive/My Drive/Datasets/Emoji/demojized_dev.txt\n"],"name":"stdout"}]},{"metadata":{"id":"hrRg2m3qG1je","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"_1cawLJh3ACH","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  NETWORK DEFINITION  ================================='''\n","\n","class RNNLM(nn.Module):\n","    def __init__(self, vocab_size: int, char_embedding_size: int,\n","                 rnn_size: int, final_output_size:int):\n","        super().__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.char_embedding_size = char_embedding_size\n","        self.rnn_size = rnn_size\n","\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                      embedding_dim=char_embedding_size)\n","\n","        self.rnn_cell = nn.LSTMCell(input_size=char_embedding_size,\n","                                   hidden_size=rnn_size)\n","        \n","        self.logits = nn.Linear(in_features=rnn_size, out_features=final_output_size)\n","        \n","        self.loss = nn.CrossEntropyLoss()\n","        \n","\n","    def get_loss(self, logits: torch.FloatTensor, y: torch.FloatTensor):\n","\n","        y = y.view(-1)\n","      \n","      \n","        #print(logits.size())\n","      \n","        return self.loss(logits, y)\n","\n","    def get_logits(self, hidden_states: torch.FloatTensor,hidden_states_indexes,\n","                   temperature: float = 1):\n","      \n","     \n","        index1 = hidden_states_indexes[:,0]\n","        index2 = hidden_states_indexes[:,1]\n","        index3 = hidden_states_indexes[:,2]\n","        \n","        a1 = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","        a2 = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","        a3 = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","        \n","        \n","        \n","        for i in range(0,_hyperparameters_dict[\"batch_size\"]):\n","          a1[i] = hidden_states[i,index1[i],:]\n","          a2[i] = hidden_states[i,index2[i],:]\n","          a3[i] = hidden_states[i,index3[i],:]\n","\n","          \n","\n","     \n","        # Modify this to change it to maxpool over hidden states\n","        newhidden = torch.max(a1,a2)\n","        newhidden = torch.max(newhidden,a3)\n","        answer = self.logits(a3)\n","\n","        return answer\n","\n","    def forward(self, x: torch.LongTensor,\n","                hidden_start: torch.FloatTensor = None, cell_start : torch.FloatTensor = None) -> torch.FloatTensor:\n","\n","\n","        max_len = x.size(1)\n","\n","        x_embedded = self.embedding(x)\n","\n","        hidden_states_list = []\n","        \n","        prev_hidden = hidden_start\n","        prev_cell = cell_start\n","        \n","        for t in range(max_len):\n","            hidden_state,hidden_cell = self.rnn_cell(x_embedded[:, t, :], (prev_hidden,prev_cell))\n","            hidden_states_list.append(hidden_state)\n","\n","            prev_hidden = hidden_state\n","            prev_cell = hidden_cell\n","\n","        hidden_states = torch.stack(hidden_states_list, dim=1)\n","\n","        return hidden_states"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D1LkG34akpJB","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"mALNHpahyhD5","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  RUN THE TRAINING / TESTING  ================================='''\n","\n","def trainRNNM(model, device, train_loader, optimizer, epoch, verbose):\n","    \n","    model.train()\n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","      \n","        hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","        data = data[:,0:_hyperparameters_dict['max_len']]\n","        data = data.to(device)\n","      \n","        data = data.type(torch.LongTensor).to(device)\n","        target = target.type(torch.LongTensor).to(device)\n","        hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","        \n","        optimizer.zero_grad()\n","\n","        hidden_states = model(data,prev_hidden,cell_hidden)        \n","        logits = model.get_logits(hidden_states,hidden_states_indexes)        \n","        loss = model.get_loss(logits,target)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(list(model.parameters()), 5.0)\n","\n","        optimizer.step()\n","\n","        hidden_states.detach()\n","\n","        if batch_idx % verbose == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                       100. * batch_idx / len(train_loader), loss.item()))\n","            # plot_loss(loss.cpu().detach().numpy(),label='train')\n","\n","            \n","def testRNNM(model, device, test_loader,verbose):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    \n","    ground_truth_array = []\n","    prediction_array = []\n","    \n","\n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    with torch.no_grad():\n","        for batch_idx,(data, target) in enumerate(test_loader):\n","            \n","            \n","            hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","            data = data[:,0:_hyperparameters_dict['max_len']]\n","            data = data.type(torch.LongTensor).to(device)\n","            \n","            target = target.type(torch.LongTensor).to(device)\n","            hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","\n","            hidden_states = model(data, prev_hidden,cell_hidden)\n","            logits = model.get_logits(hidden_states,hidden_states_indexes)\n","            loss = model.get_loss(logits, target)\n","\n","\n","            hidden_states.detach()\n","\n","            test_loss += loss\n","            pred = logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            \n","            \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","            prediction_array.extend(pred.view(-1)[:].cpu().numpy())\n","            ground_truth_array.extend(target[:].cpu().numpy())\n","          \n","            \n","\n","   \n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    #plot_loss(test_loss,label='test',color='red')\n","    \n","        \n","    print(classification_report(ground_truth_array, prediction_array, target_names=label_dict.getlabellist()))\n","    \n","    \n","    print('\\n')\n","    \n","    plot_confusion_matrix(ground_truth_array, prediction_array, classes=label_dict.getlabellist(),\n","                      title='Confusion matrix')\n","    \n","            "],"execution_count":0,"outputs":[]},{"metadata":{"id":"4YYZIJbCdBL0","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  HYPERPARAMETERS  ================================='''\n","\n","_hyperparameters_dict = {\n","    \"batch_size\": 32,\n","    \"num_epochs\": 15,\n","    \"max_len\": 200,\n","    \"embedding_size\": 100,\n","    \"rnn_size\": 512,\n","    \"learning_algo\": \"adam\",\n","    \"learning_rate\": 0.0005,\n","    \"output_size\" : 2,\n","    \"max_grad_norm\": 5.0\n","}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mBVKsNdRkxI0","colab_type":"code","outputId":"bfb69018-162c-4429-8413-ba1608a43249","executionInfo":{"status":"error","timestamp":1555247500871,"user_tz":-180,"elapsed":727,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":247}},"cell_type":"code","source":["'''=================================  RUN THE TRAINING / TESTING  ================================='''\n","\n","network= RNNLM(vocab.size(),_hyperparameters_dict['embedding_size'], _hyperparameters_dict['rnn_size'], _hyperparameters_dict['output_size'])\n","network = network.to(device)\n","\n","epochs = _hyperparameters_dict['num_epochs']\n","verbose = 100\n","\n","\n","optimizer = torch.optim.Adam(network.parameters(),lr=_hyperparameters_dict['learning_rate'])\n","\n","print(label_dict.getlabellist())\n","for epoch in range(1, epochs):\n","    trainRNNM(network,device,data_loader_train,optimizer,epoch, verbose)\n","    testRNNM(network, device, data_loader_test, verbose)\n","  "],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5faf7607a23c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'''=================================  RUN THE TRAINING / TESTING  ================================='''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_hyperparameters_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hyperparameters_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnn_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hyperparameters_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'RNNLM' is not defined"]}]},{"metadata":{"id":"45NPYB6GFcyP","colab_type":"code","colab":{}},"cell_type":"code","source":["for batch_idx,(data,target) in enumerate(data_loader_train):\n","  print(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pAqB4F8eFmuo","colab_type":"code","colab":{}},"cell_type":"code","source":["for batch_idx,(data,target) in enumerate(data_loader_test):\n","  print(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dti3C-vY-li4","colab_type":"code","colab":{}},"cell_type":"code","source":["label_dict.printitems()"],"execution_count":0,"outputs":[]}]}