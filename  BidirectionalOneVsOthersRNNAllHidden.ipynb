{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" BidirectionalOneVsOthersRNNAllHidden.ipynb","version":"0.3.2","provenance":[{"file_id":"10UkIez0cFm8Qkgmk3inWBQkZ9Ie37yI1","timestamp":1555247351959},{"file_id":"1T9nURBMhAT64kaN8gehL1Qh2SJpizQcl","timestamp":1555242971986},{"file_id":"1BC8eJ8arIEy7d7_t2hxk3wk5rCBoFFL2","timestamp":1555164346169},{"file_id":"1LPiZl2F6SRNUQ3VDyHR91mSwMoJW7h-9","timestamp":1555161651655},{"file_id":"1MwJSWKyMObQnQscRIRa0YJn5urCEbkRR","timestamp":1555156242884},{"file_id":"1MwJSWKyMObQnQscRIRa0YJn5urCEbkRR","timestamp":1555152441220},{"file_id":"1ZIUZC-sPimWy1RSqtNoNEG7XX_yqNi71","timestamp":1554971018806}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WETg5r8EhbVo","colab_type":"code","outputId":"707df637-fda5-4a75-8732-4f920be92444","executionInfo":{"status":"ok","timestamp":1555247569259,"user_tz":-180,"elapsed":559,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["import json\n","import torch\n","import re\n","import numpy as np\n","import collections\n","\n","from torch import nn\n","from torch.optim import SGD, Adam\n","from google.colab import files, auth, drive\n","from urllib.request import urlopen\n","from typing import List, Dict, Callable\n","from collections import Counter\n","from os import path\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import classification_report\n","drive.mount('/content/gdrive')\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","cuda\n"],"name":"stdout"}]},{"metadata":{"id":"QH0UNqvjmh6C","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  DEFINES  ================================='''\n","\n","'''======= Paths ======='''\n","\n","TrainPath = \"gdrive/My Drive/Datasets/Emoji/demojized_train.txt\"\n","TestPath = \"gdrive/My Drive/Datasets/Emoji/demojized_dev.txt\"\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-2oXk7VZSA-4","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  EXTERNAL FUNCTIONS  ================================='''\n","\n","\n","def readfile(filepath: str) -> str:\n","    \"\"\"\n","    Reads file and returns its content as a string.\n","    \"\"\"\n","    #response = urlopen(url)\n","    #body = response.read().decode('utf-8')\n","    with open(filepath, \"r\") as f:\n","        body = f.read()\n","    \n","    return body.encode('ascii', 'ignore').decode(\"utf-8\")\n","\n","\n","# Used with regex\n","\n","def find_all(a_str, sub):\n","    start = 0\n","    while True:\n","        start = a_str.find(sub, start)\n","        if start == -1: return\n","        yield start\n","        start += len(sub) \n","  \n","\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    #print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7EmcK3GrlcPj","colab_type":"code","colab":{}},"cell_type":"code","source":["'''================================= CUSTOM DATA-STRUCTURES  ================================='''\n","\n","class OneVsOthersDict():\n","  \n","  def __init__(self,):\n","      self.to_number = collections.OrderedDict()\n","      self.to_language = collections.OrderedDict()\n","    \n","  def insert(self,word):    \n","    if word == 'others':\n","      self.to_number[word] = 0\n","      self.to_language[0] = word\n","    else:\n","      self.to_number[word] = 1\n","      self.to_language[1] = 'feelings'\n","        \n","        \n","  def printitems(self):\n","      for label,value in self.to_number.items():\n","        print(label + ' ' + str(value))\n","        \n","  def getindex(self,word):\n","    return self.to_number[word]\n","  \n","  def getlabel(self,label):\n","    return self.to_language[label]\n","  \n","  def getlabellist(self):\n","    label_list = []\n","    for label,value in self.to_language.items():\n","      label_list.append(value)\n","    \n","    return label_list\n","  \n","  \n","#Mostly pentru label-uri\n","class TwoWayDict():\n","  \n","  def __init__(self,):\n","    self.to_number = collections.OrderedDict()\n","    self.to_language = collections.OrderedDict()\n","    \n","\n","    \n","  def insert(self,word):\n","    if not word in self.to_number:\n","      new_index = len(self.to_number)\n","      self.to_number[word] = new_index\n","      self.to_language[new_index] = word\n","    \n","    \n","  def getindex(self,word):\n","    return self.to_number[word]\n","  \n","  def getlabel(self,label):\n","    return self.to_language[label]\n","  \n","  def getlabellist(self):\n","    label_list = []\n","    for label,value in self.to_language.items():\n","      label_list.append(value)\n","    \n","    return label_list\n","\n","  \n","#  Pentru vocabular de tip char  \n","class Vocabulary:\n","    \"\"\"\n","    Helper class that maps characters to unique indices and the other way around\n","    \"\"\"\n","    def __init__(self, text: str):\n","        #special character for padding shorter sequences in a mini-batches\n","        characters_set = set(\"Â©\") \n","        characters_set.update(text)\n","\n","        \n","        self.char_to_idx = {char:idx for (idx, char) \n","                            in enumerate(characters_set)}\n","        \n","       \n","        self.idx_to_char = {idx:char for (idx, char) \n","                            in enumerate(characters_set)}\n","   \n","    def size(self):\n","        return len(self.char_to_idx)\n","      \n","    def __str__(self):\n","        return str(self.char_to_idx)\n","  \n","  \n","def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n","    \"\"\"\n","    Convert a string to a Tensor with corresponding character indices\n","    e.g. \"We have\" -> [12, 6, 20, 13, 1, 25, 6] \n","    \"\"\"\n","    \n","    \n","    text_indices = [vocab.char_to_idx[c] for c in text]\n","    \n","    return torch.tensor(text_indices)  \n","\n","  \n","def tensor_to_text(x: torch.LongTensor, vocab: Vocabulary) -> str:\n","    \"\"\"\n","    Convert a Tensor of character indices to its string representation\n","    e.g. [12, 6, 20, 13, 1, 25, 6] -> \"We have\"\n","    \"\"\"\n","    \n","    x = x.cpu().detach().numpy()\n","    \n","    for elem in x :\n","      print(vocab.idx_to_char[elem],sep='')\n","\n","  \n","def get_vocabulary(trainPath : str):\n","    \n","    text = readfile(trainPath)\n","    \n","    vocab = Vocabulary(text)\n","    \n","    return vocab\n","\n","\n","    \n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pwMshf8hhnY3","colab_type":"code","outputId":"6864044e-f7c2-467e-b22e-a8ad1d8ca7d8","executionInfo":{"status":"ok","timestamp":1555247570048,"user_tz":-180,"elapsed":1267,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"cell_type":"code","source":["'''=================================   GLOVE EMBEDDING  ================================='''\n","\n","'''====== Load Glove Model======'''\n","\n","def loadGloveModel(gloveFile):\n","    print(\"Loading Glove Model\")\n","    f = open(gloveFile,'r')\n","    model = {}\n","    for line in f:\n","        splitLine = line.split()\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print(\"Done.\",len(model),\" words loaded!\")\n","    return model\n","\n","  \n","'''====== Get Word Embeddings ======'''\n","\n","def get_value(word,gloveModel):\n","  answer = torch.zeros(100,dtype=torch.float64)  \n","  if word in gloveModel.keys():\n","    answer = answer + torch.tensor(gloveModel[word],dtype=torch.float64)  \n","  return answer\n","\n","\n","def get_tensor_form(phrase,gloveModel):  \n","  \n","  phrase_tensor = get_value(phrase[0],gloveModel)\n","  for i in range(1,len(phrase)):\n","    phrase_tensor += get_value(phrase[i], gloveModel)\n","\n","  return phrase_tensor  \n","  \n","  \n","  \n","'''====== Instantiate Glove Model ======'''\n","  \n","  \n","#glovePath = \"gdrive/My Drive/Datasets/Emoji/glove.twitter.27B.100d.txt\"\n","#gloveModel = loadGloveModel(glovePath)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'====== Instantiate Glove Model ======'"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"6Hgm8lUEyCgm","colab_type":"code","colab":{}},"cell_type":"code","source":["def cut_phrase(phrase, max_length):\n","\n","  \n","  phrase_bounds = list(find_all(phrase, '<end>'))\n","\n","  leftsentence = \"\"\n","  middlesentence = \"\"\n","  rightsentence = \"\"\n","        \n","  for i in range(phrase_bounds[0],phrase_bounds[1]):\n","    leftsentence = leftsentence + phrase[i]\n","  \n","  for i in range(phrase_bounds[1],phrase_bounds[2]):\n","    middlesentence = middlesentence + phrase[i]\n","\n","  for i in range(phrase_bounds[2],phrase_bounds[3]):\n","    rightsentence = rightsentence + phrase[i]\n","\n","  \n","  l1 = len(leftsentence)\n","  l2 = len(middlesentence)\n","  l3 = len(rightsentence)\n","  \n","  '''\n","  -8 deoarece de la ultima se taie fix <end> si vrem sa adaugam si un spatiu\n","  de asemenea se modifica cea din mijloc si la stanga si la dreapta\n","  '''\n","  \n","  while(l1+l2+l3 > max_length - 8):\n","    \n","    if(l1 >= l2 and l1 >= l3):\n","      leftsentence = leftsentence[:-1]\n","      l1 -= 1\n","    elif(l2 >= l1 and l2 >= l3):\n","      middlesentence = middlesentence[:-1]\n","      l2 -= 1\n","    else:\n","      rightsentence = rightsentence[:-1]\n","      l3 -= 1\n","    \n","  middlesentence = \" \" + middlesentence + \" \"\n","  rightsentence = rightsentence + \" <end>\"\n","  \n","  #print(leftsentence)\n","    \n","  return leftsentence + middlesentence + rightsentence  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_MpcBPm5m69f","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================   DATA LOADING  ================================='''\n","def get_data(path : str,  label_dict: TwoWayDict):\n","  print(path)\n","  labelcounter = 0\n","  words = []\n","  labels = []\n","  with open(path) as file:\n","    cnt = 0 \n","    for line in file:\n","      if cnt > 0:\n","        label = line.split()[-1]\n","        \n","        labels.append(label)\n","        \n","        templine = line.split(' ')[1:-1]\n","        \n","        words.append(templine)\n","\n","        label_dict.insert(label)\n","\n","      cnt+=1\n","  \n","  return [words,labels]\n","  \n","  \n","  \n","def get_dataset_loader(words, labels, label_dict, vocabulary, max_length=95,max_batch_idx=100, shuffle=True):\n","  \n","  dataset_length = len(labels)\n","\n","  traintensor = torch.zeros(dataset_length,max_length + 3) # Cele 3 hidden state-uri pe care le cautam :)\n","  labeltensor = torch.zeros(dataset_length,1)\n","  \n","  padding_tensor = torch.tensor([vocabulary.char_to_idx[\"Â©\"]])\n","  \n","  for i in range(0, dataset_length):\n","\n","    \n","    phrase = words[i]\n","    currentlabel = labels[i]  \n","    \n","    phrase = ' '.join(phrase)\n","    \n","    if(len(phrase) > max_length):\n","      phrase = cut_phrase(phrase,max_length)\n","    \n","    hidden_state_positions = []\n","    \n","    \n","    hidden_state_positions = list(find_all(phrase, '<end>'))\n","    hidden_state_positions = [x+4 for x in hidden_state_positions]\n","    hidden_state_positions.pop(0) # delete the first <end>\n","    \n","    \n","    #print(hidden_state_positions)\n","    #print(phrase)\n","    \n","    phrase = text_to_tensor(phrase,vocabulary)\n","    \n","    while(phrase.size()[0] < max_length):\n","      phrase = torch.cat((phrase,padding_tensor))\n","    \n","    currentlabel = label_dict.getindex(currentlabel)\n","\n","    \n","    hidden_state_positions = torch.tensor(hidden_state_positions)\n","    #print(hidden_state_positions)\n","    \n","    phrase = torch.cat((phrase,hidden_state_positions))\n","    phrase = phrase.to(device)\n","    \n","    #print(phrase.size())\n","    \n","    newlabel = torch.zeros(1).to(device).long()\n","    newlabel[0]=currentlabel\n","    currentlabel = newlabel\n","\n","    traintensor[i] = phrase\n","    labeltensor[i] = currentlabel\n","    \n","  \n","  large_dataset = TensorDataset(torch.tensor(traintensor).to(device), torch.tensor(labeltensor).to(device))\n","  large_data_loader = DataLoader(large_dataset, batch_size=max_batch_idx, shuffle=True, drop_last=True)\n","  return large_data_loader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xGQ5LFrVD2E1","colab_type":"code","outputId":"cfc03eb9-8199-44de-d7d5-27b08bc6cb3d","executionInfo":{"status":"ok","timestamp":1555247645315,"user_tz":-180,"elapsed":76481,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"cell_type":"code","source":["label_dict = OneVsOthersDict()\n","vocab = get_vocabulary(TrainPath)\n","\n","[trainingwords,traininglabels] = get_data(TrainPath,label_dict)\n","data_loader_train = get_dataset_loader(trainingwords,traininglabels,label_dict, vocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['batch_size'])\n","\n","[testingwords,testinglabels] = get_data(TestPath,label_dict)\n","data_loader_test  = get_dataset_loader(testingwords,testinglabels,label_dict, vocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['batch_size'])\n","\n","'''\n","Am testat sa vad daca datele sunt create in mod corespunzator\n","\n","for batch_idx, (data, target) in enumerate(data_loader_test):\n","  hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","  data = data[:,0:_hyperparameters_dict['max_len']]\n","  text = tensor_to_text(data[0,:],vocab)\n","  break\n","'''\n","    "],"execution_count":22,"outputs":[{"output_type":"stream","text":["gdrive/My Drive/Datasets/Emoji/demojized_train.txt\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["gdrive/My Drive/Datasets/Emoji/demojized_dev.txt\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["\"\\nAm testat sa vad daca datele sunt create in mod corespunzator\\n\\nfor batch_idx, (data, target) in enumerate(data_loader_test):\\n  hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\\n  data = data[:,0:_hyperparameters_dict['max_len']]\\n  text = tensor_to_text(data[0,:],vocab)\\n  break\\n\""]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"hrRg2m3qG1je","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"_1cawLJh3ACH","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  NETWORK DEFINITION  ================================='''\n","\n","class RNNLM(nn.Module):\n","    def __init__(self, vocab_size: int, char_embedding_size: int,\n","                 rnn_size: int, final_output_size:int):\n","        super().__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.char_embedding_size = char_embedding_size\n","        self.rnn_size = rnn_size\n","\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                      embedding_dim=char_embedding_size)\n","\n","        self.rnn_cell = nn.LSTMCell(input_size=char_embedding_size,\n","                                   hidden_size=rnn_size)\n","        \n","        self.rrn_cell_reverse = nn.LSTMCell(input_size=char_embedding_size,\n","                                   hidden_size=rnn_size)\n","        \n","        \n","        self.logits = nn.Linear(in_features=rnn_size*2, out_features=final_output_size)\n","        \n","        self.loss = nn.CrossEntropyLoss()\n","        \n","\n","    def get_loss(self, logits: torch.FloatTensor, y: torch.FloatTensor):\n","\n","        y = y.view(-1)\n","      \n","      \n","        #print(logits.size())\n","      \n","        return self.loss(logits, y)\n","\n","    def get_logits(self, hidden_states: torch.FloatTensor,hidden_states_reverse : torch.FloatTensor, hidden_states_indexes,\n","                   temperature: float = 1):\n","      \n","        max_len = hidden_states.size(1)\n","        \n","        hidden_state_max = torch.max(hidden_states,dim=1)[0]\n","        hidden_state_max_reverse = torch.max(hidden_states_reverse,dim=1)[0]\n","        \n","        answer = torch.cat((hidden_state_max.type(torch.FloatTensor),hidden_state_max_reverse.type(torch.FloatTensor)),dim=1).to(device)\n","\n","        return self.logits(answer)        \n","        \n","\n","    def forward(self, x: torch.LongTensor,\n","                hidden_start: torch.FloatTensor = None, cell_start : torch.FloatTensor = None,hidden_start_reverse : torch.FloatTensor = None, cell_start_reverse : torch.FloatTensor = None) -> torch.FloatTensor:\n","\n","\n","        max_len = x.size(1)\n","\n","        x_embedded = self.embedding(x)\n","\n","        hidden_states_list = []\n","        hidden_states_list_reverse = []\n","        \n","        prev_hidden = hidden_start\n","        prev_cell = cell_start\n","       \n","        prev_hidden_reverse = hidden_start_reverse\n","        prev_cell_reverse = cell_start_reverse\n","        \n","        \n","        \n","        for t in range(max_len):\n","\n","            hidden_state,hidden_cell = self.rnn_cell(x_embedded[:, t, :], (prev_hidden,prev_cell))\n","            hidden_states_list.append(hidden_state)\n","\n","            prev_hidden = hidden_state\n","            prev_cell = hidden_cell\n","\n","        hidden_states = torch.stack(hidden_states_list, dim=1).to(device)\n","        \n","        \n","        for t in range(max_len):\n","\n","            hidden_state_reverse,hidden_cell_reverse = self.rrn_cell_reverse( x_embedded[:,max_len-t-1,:], (prev_hidden_reverse,prev_cell_reverse))\n","            hidden_states_list_reverse.append(hidden_state_reverse)            \n","            \n","            prev_hidden_reverse = hidden_state_reverse\n","            prev_cell_reverse = hidden_cell_reverse\n","        \n","        hidden_states_reverse = torch.stack(hidden_states_list_reverse,dim=1).to(device)\n","        \n","        \n","        return [hidden_states,hidden_states_reverse]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D1LkG34akpJB","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"mALNHpahyhD5","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  RUN THE TRAINING / TESTING  ================================='''\n","\n","def trainRNNM(model, device, train_loader, optimizer, epoch, verbose):\n","    \n","    model.train()\n","    \n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_reverse = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_reverse = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","      \n","        hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","        \n","        # Fac asta deoarece in ultimile 3 valori din sample-ul ales sunt de fapt hidden-state-urile care corespund cuvintelor : <end>.\n","        # Asta doar ca sa nu imi fac functia proprie ptr get_batch_sample. reusing code ftw\n","        data = data[:,0:_hyperparameters_dict['max_len']]\n","        data = data.to(device)\n","      \n","        data = data.type(torch.LongTensor).to(device)\n","        target = target.type(torch.LongTensor).to(device)\n","        hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","        \n","        optimizer.zero_grad()\n","\n","        [hidden_states,hidden_states_reverse] = model(data,prev_hidden,cell_hidden,prev_hidden_reverse,cell_hidden_reverse)        \n","        logits = model.get_logits(hidden_states,hidden_states_reverse,hidden_states_indexes)        \n","        loss = model.get_loss(logits,target)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(list(model.parameters()), 5.0)\n","\n","        optimizer.step()\n","\n","        hidden_states.detach()\n","        hidden_states_reverse.detach()\n","\n","        if batch_idx % verbose == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                       100. * batch_idx / len(train_loader), loss.item()))\n","            # plot_loss(loss.cpu().detach().numpy(),label='train')\n","\n","            \n","def testRNNM(model, device, test_loader,verbose):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    \n","    ground_truth_array = []\n","    prediction_array = []\n","    \n","\n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    \n","    prev_hidden_reverse = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_reverse = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    with torch.no_grad():\n","        for batch_idx,(data, target) in enumerate(test_loader):\n","            \n","            hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","            \n","            # Fac asta deoarece in ultimile 3 valori din sample-ul ales sunt de fapt hidden-state-urile care corespund cuvintelor : <end>.\n","            # Asta doar ca sa nu imi fac functia proprie ptr get_batch_sample. reusing code ftw\n","            data = data[:,0:_hyperparameters_dict['max_len']]\n","            data = data.type(torch.LongTensor).to(device)\n","\n","            target = target.type(torch.LongTensor).to(device)\n","            hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","\n","            [hidden_states,hidden_states_reverse] = model(data,prev_hidden,cell_hidden,prev_hidden_reverse,cell_hidden_reverse)        \n","            logits = model.get_logits(hidden_states,hidden_states_reverse,hidden_states_indexes)        \n","            loss = model.get_loss(logits,target)\n","\n","\n","            hidden_states.detach()\n","\n","            test_loss += loss\n","            pred = logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            \n","            \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","            prediction_array.extend(pred.view(-1)[:].cpu().numpy())\n","            ground_truth_array.extend(target[:].cpu().numpy())\n","          \n","            \n","\n","   \n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    #plot_loss(test_loss,label='test',color='red')\n","    \n","        \n","    print(classification_report(ground_truth_array, prediction_array, target_names=label_dict.getlabellist()))\n","    \n","    \n","    print('\\n')\n","    \n","    plot_confusion_matrix(ground_truth_array, prediction_array, classes=label_dict.getlabellist(),\n","                      title='Confusion matrix')\n","    \n","            "],"execution_count":0,"outputs":[]},{"metadata":{"id":"4YYZIJbCdBL0","colab_type":"code","colab":{}},"cell_type":"code","source":["##'''=================================  HYPERPARAMETERS  ================================='''\n","\n","_hyperparameters_dict = {\n","    \"batch_size\": 32,\n","    \"num_epochs\": 1000,\n","    \"max_len\": 200,\n","    \"embedding_size\": 100,\n","    \"rnn_size\": 512,\n","    \"learning_algo\": \"adam\",\n","    \"learning_rate\": 0.0001,\n","    \"output_size\" : 2,\n","    \"max_grad_norm\": 5.0\n","}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mBVKsNdRkxI0","colab_type":"code","outputId":"d03c6de6-5845-482a-aafc-3f8b95defb84","executionInfo":{"status":"error","timestamp":1555165245820,"user_tz":-180,"elapsed":1074,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":14017}},"cell_type":"code","source":["'''=================================  RUN THE TRAINING / TESTING  ================================='''\n","\n","network= RNNLM(vocab.size(),_hyperparameters_dict['embedding_size'], _hyperparameters_dict['rnn_size'], _hyperparameters_dict['output_size'])\n","network = network.to(device)\n","\n","epochs = _hyperparameters_dict['num_epochs']\n","verbose = 100\n","\n","\n","optimizer = torch.optim.Adam(network.parameters(),lr=_hyperparameters_dict['learning_rate'])\n","\n","print(label_dict.getlabellist())\n","for epoch in range(1, epochs):\n","    trainRNNM(network,device,data_loader_train,optimizer,epoch, verbose)\n","    testRNNM(network, device, data_loader_test, verbose)\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["['others', 'feelings']\n","Train Epoch: 1 [0/30160 (0%)]\tLoss: 0.668187\n","Train Epoch: 1 [3200/30160 (11%)]\tLoss: 0.679761\n","Train Epoch: 1 [6400/30160 (21%)]\tLoss: 0.612283\n","Train Epoch: 1 [9600/30160 (32%)]\tLoss: 0.559239\n","Train Epoch: 1 [12800/30160 (42%)]\tLoss: 0.623239\n","Train Epoch: 1 [16000/30160 (53%)]\tLoss: 0.522574\n","Train Epoch: 1 [19200/30160 (64%)]\tLoss: 0.484822\n","Train Epoch: 1 [22400/30160 (74%)]\tLoss: 0.523737\n","Train Epoch: 1 [25600/30160 (85%)]\tLoss: 0.513001\n","Train Epoch: 1 [28800/30160 (96%)]\tLoss: 0.490869\n","\n","Test set: Average loss: 0.0156, Accuracy: 2162/2755 (78%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.95      0.79      0.86      2335\n","    feelings       0.40      0.79      0.53       417\n","\n","   micro avg       0.79      0.79      0.79      2752\n","   macro avg       0.67      0.79      0.69      2752\n","weighted avg       0.87      0.79      0.81      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 2 [0/30160 (0%)]\tLoss: 0.445874\n","Train Epoch: 2 [3200/30160 (11%)]\tLoss: 0.379833\n","Train Epoch: 2 [6400/30160 (21%)]\tLoss: 0.399035\n","Train Epoch: 2 [9600/30160 (32%)]\tLoss: 0.419850\n","Train Epoch: 2 [12800/30160 (42%)]\tLoss: 0.352032\n","Train Epoch: 2 [16000/30160 (53%)]\tLoss: 0.398046\n","Train Epoch: 2 [19200/30160 (64%)]\tLoss: 0.472188\n","Train Epoch: 2 [22400/30160 (74%)]\tLoss: 0.440587\n","Train Epoch: 2 [25600/30160 (85%)]\tLoss: 0.546727\n","Train Epoch: 2 [28800/30160 (96%)]\tLoss: 0.384682\n","\n","Test set: Average loss: 0.0135, Accuracy: 2239/2755 (81%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.81      0.88      2336\n","    feelings       0.44      0.84      0.58       416\n","\n","   micro avg       0.81      0.81      0.81      2752\n","   macro avg       0.70      0.82      0.73      2752\n","weighted avg       0.89      0.81      0.83      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 3 [0/30160 (0%)]\tLoss: 0.340124\n","Train Epoch: 3 [3200/30160 (11%)]\tLoss: 0.360957\n","Train Epoch: 3 [6400/30160 (21%)]\tLoss: 0.319222\n","Train Epoch: 3 [9600/30160 (32%)]\tLoss: 0.262637\n","Train Epoch: 3 [12800/30160 (42%)]\tLoss: 0.300201\n","Train Epoch: 3 [16000/30160 (53%)]\tLoss: 0.333314\n","Train Epoch: 3 [19200/30160 (64%)]\tLoss: 0.298570\n","Train Epoch: 3 [22400/30160 (74%)]\tLoss: 0.221629\n","Train Epoch: 3 [25600/30160 (85%)]\tLoss: 0.328379\n","Train Epoch: 3 [28800/30160 (96%)]\tLoss: 0.271315\n","\n","Test set: Average loss: 0.0166, Accuracy: 2094/2755 (76%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.98      0.73      0.84      2335\n","    feelings       0.38      0.91      0.54       417\n","\n","   micro avg       0.76      0.76      0.76      2752\n","   macro avg       0.68      0.82      0.69      2752\n","weighted avg       0.89      0.76      0.79      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 4 [0/30160 (0%)]\tLoss: 0.297794\n","Train Epoch: 4 [3200/30160 (11%)]\tLoss: 0.298924\n","Train Epoch: 4 [6400/30160 (21%)]\tLoss: 0.327977\n","Train Epoch: 4 [9600/30160 (32%)]\tLoss: 0.252207\n","Train Epoch: 4 [12800/30160 (42%)]\tLoss: 0.332618\n","Train Epoch: 4 [16000/30160 (53%)]\tLoss: 0.412201\n","Train Epoch: 4 [19200/30160 (64%)]\tLoss: 0.215032\n","Train Epoch: 4 [22400/30160 (74%)]\tLoss: 0.233638\n","Train Epoch: 4 [25600/30160 (85%)]\tLoss: 0.251227\n","Train Epoch: 4 [28800/30160 (96%)]\tLoss: 0.256400\n","\n","Test set: Average loss: 0.0096, Accuracy: 2431/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.90      0.93      2335\n","    feelings       0.58      0.80      0.68       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.85      0.80      2752\n","weighted avg       0.90      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 5 [0/30160 (0%)]\tLoss: 0.269155\n","Train Epoch: 5 [3200/30160 (11%)]\tLoss: 0.162576\n","Train Epoch: 5 [6400/30160 (21%)]\tLoss: 0.281941\n","Train Epoch: 5 [9600/30160 (32%)]\tLoss: 0.355906\n","Train Epoch: 5 [12800/30160 (42%)]\tLoss: 0.295509\n","Train Epoch: 5 [16000/30160 (53%)]\tLoss: 0.169481\n","Train Epoch: 5 [19200/30160 (64%)]\tLoss: 0.315360\n","Train Epoch: 5 [22400/30160 (74%)]\tLoss: 0.227829\n","Train Epoch: 5 [25600/30160 (85%)]\tLoss: 0.337159\n","Train Epoch: 5 [28800/30160 (96%)]\tLoss: 0.182726\n","\n","Test set: Average loss: 0.0096, Accuracy: 2437/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.90      0.93      2336\n","    feelings       0.59      0.83      0.69       416\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.86      0.81      2752\n","weighted avg       0.91      0.89      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 6 [0/30160 (0%)]\tLoss: 0.186837\n","Train Epoch: 6 [3200/30160 (11%)]\tLoss: 0.231635\n","Train Epoch: 6 [6400/30160 (21%)]\tLoss: 0.102094\n","Train Epoch: 6 [9600/30160 (32%)]\tLoss: 0.257250\n","Train Epoch: 6 [12800/30160 (42%)]\tLoss: 0.124531\n","Train Epoch: 6 [16000/30160 (53%)]\tLoss: 0.138640\n","Train Epoch: 6 [19200/30160 (64%)]\tLoss: 0.116597\n","Train Epoch: 6 [22400/30160 (74%)]\tLoss: 0.170598\n","Train Epoch: 6 [25600/30160 (85%)]\tLoss: 0.281340\n","Train Epoch: 6 [28800/30160 (96%)]\tLoss: 0.320702\n","\n","Test set: Average loss: 0.0097, Accuracy: 2424/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.89      0.93      2335\n","    feelings       0.57      0.84      0.68       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.87      0.80      2752\n","weighted avg       0.91      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 7 [0/30160 (0%)]\tLoss: 0.197186\n","Train Epoch: 7 [3200/30160 (11%)]\tLoss: 0.190776\n","Train Epoch: 7 [6400/30160 (21%)]\tLoss: 0.205006\n","Train Epoch: 7 [9600/30160 (32%)]\tLoss: 0.217503\n","Train Epoch: 7 [12800/30160 (42%)]\tLoss: 0.106506\n","Train Epoch: 7 [16000/30160 (53%)]\tLoss: 0.163899\n","Train Epoch: 7 [19200/30160 (64%)]\tLoss: 0.207526\n","Train Epoch: 7 [22400/30160 (74%)]\tLoss: 0.163633\n","Train Epoch: 7 [25600/30160 (85%)]\tLoss: 0.128434\n","Train Epoch: 7 [28800/30160 (96%)]\tLoss: 0.168536\n","\n","Test set: Average loss: 0.0108, Accuracy: 2388/2755 (87%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.87      0.92      2335\n","    feelings       0.54      0.88      0.67       417\n","\n","   micro avg       0.87      0.87      0.87      2752\n","   macro avg       0.76      0.87      0.79      2752\n","weighted avg       0.91      0.87      0.88      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 8 [0/30160 (0%)]\tLoss: 0.100500\n","Train Epoch: 8 [3200/30160 (11%)]\tLoss: 0.052673\n","Train Epoch: 8 [6400/30160 (21%)]\tLoss: 0.084224\n","Train Epoch: 8 [9600/30160 (32%)]\tLoss: 0.180054\n","Train Epoch: 8 [12800/30160 (42%)]\tLoss: 0.066691\n","Train Epoch: 8 [16000/30160 (53%)]\tLoss: 0.180498\n","Train Epoch: 8 [19200/30160 (64%)]\tLoss: 0.230117\n","Train Epoch: 8 [22400/30160 (74%)]\tLoss: 0.250065\n","Train Epoch: 8 [25600/30160 (85%)]\tLoss: 0.161575\n","Train Epoch: 8 [28800/30160 (96%)]\tLoss: 0.194812\n","\n","Test set: Average loss: 0.0098, Accuracy: 2422/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.89      0.93      2335\n","    feelings       0.57      0.85      0.68       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.87      0.80      2752\n","weighted avg       0.91      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 9 [0/30160 (0%)]\tLoss: 0.314994\n","Train Epoch: 9 [3200/30160 (11%)]\tLoss: 0.098091\n","Train Epoch: 9 [6400/30160 (21%)]\tLoss: 0.233689\n","Train Epoch: 9 [9600/30160 (32%)]\tLoss: 0.340598\n","Train Epoch: 9 [12800/30160 (42%)]\tLoss: 0.302874\n","Train Epoch: 9 [16000/30160 (53%)]\tLoss: 0.160248\n","Train Epoch: 9 [19200/30160 (64%)]\tLoss: 0.088191\n","Train Epoch: 9 [22400/30160 (74%)]\tLoss: 0.094729\n","Train Epoch: 9 [25600/30160 (85%)]\tLoss: 0.260964\n","Train Epoch: 9 [28800/30160 (96%)]\tLoss: 0.331025\n","\n","Test set: Average loss: 0.0092, Accuracy: 2443/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.90      0.93      2335\n","    feelings       0.59      0.83      0.69       417\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.86      0.81      2752\n","weighted avg       0.91      0.89      0.90      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 10 [0/30160 (0%)]\tLoss: 0.110207\n","Train Epoch: 10 [3200/30160 (11%)]\tLoss: 0.068329\n","Train Epoch: 10 [6400/30160 (21%)]\tLoss: 0.132295\n","Train Epoch: 10 [9600/30160 (32%)]\tLoss: 0.270353\n","Train Epoch: 10 [12800/30160 (42%)]\tLoss: 0.158074\n","Train Epoch: 10 [16000/30160 (53%)]\tLoss: 0.176876\n","Train Epoch: 10 [19200/30160 (64%)]\tLoss: 0.071334\n","Train Epoch: 10 [22400/30160 (74%)]\tLoss: 0.320073\n","Train Epoch: 10 [25600/30160 (85%)]\tLoss: 0.180844\n","Train Epoch: 10 [28800/30160 (96%)]\tLoss: 0.225063\n","\n","Test set: Average loss: 0.0084, Accuracy: 2484/2755 (90%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.92      0.94      2337\n","    feelings       0.64      0.81      0.71       415\n","\n","   micro avg       0.90      0.90      0.90      2752\n","   macro avg       0.80      0.86      0.83      2752\n","weighted avg       0.92      0.90      0.91      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 11 [0/30160 (0%)]\tLoss: 0.116812\n","Train Epoch: 11 [3200/30160 (11%)]\tLoss: 0.173307\n","Train Epoch: 11 [6400/30160 (21%)]\tLoss: 0.195869\n","Train Epoch: 11 [9600/30160 (32%)]\tLoss: 0.460238\n","Train Epoch: 11 [12800/30160 (42%)]\tLoss: 0.103637\n","Train Epoch: 11 [16000/30160 (53%)]\tLoss: 0.173511\n","Train Epoch: 11 [19200/30160 (64%)]\tLoss: 0.140217\n","Train Epoch: 11 [22400/30160 (74%)]\tLoss: 0.077221\n","Train Epoch: 11 [25600/30160 (85%)]\tLoss: 0.076210\n","Train Epoch: 11 [28800/30160 (96%)]\tLoss: 0.117965\n","\n","Test set: Average loss: 0.0093, Accuracy: 2456/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.91      0.93      2335\n","    feelings       0.61      0.82      0.70       417\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.79      0.86      0.82      2752\n","weighted avg       0.91      0.89      0.90      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 12 [0/30160 (0%)]\tLoss: 0.137158\n","Train Epoch: 12 [3200/30160 (11%)]\tLoss: 0.159680\n","Train Epoch: 12 [6400/30160 (21%)]\tLoss: 0.312701\n","Train Epoch: 12 [9600/30160 (32%)]\tLoss: 0.207964\n","Train Epoch: 12 [12800/30160 (42%)]\tLoss: 0.112218\n","Train Epoch: 12 [16000/30160 (53%)]\tLoss: 0.161724\n","Train Epoch: 12 [19200/30160 (64%)]\tLoss: 0.130692\n","Train Epoch: 12 [22400/30160 (74%)]\tLoss: 0.050204\n","Train Epoch: 12 [25600/30160 (85%)]\tLoss: 0.155678\n","Train Epoch: 12 [28800/30160 (96%)]\tLoss: 0.039025\n","\n","Test set: Average loss: 0.0094, Accuracy: 2445/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.90      0.93      2335\n","    feelings       0.60      0.82      0.69       417\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.86      0.81      2752\n","weighted avg       0.91      0.89      0.90      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 13 [0/30160 (0%)]\tLoss: 0.094788\n","Train Epoch: 13 [3200/30160 (11%)]\tLoss: 0.161996\n","Train Epoch: 13 [6400/30160 (21%)]\tLoss: 0.220931\n","Train Epoch: 13 [9600/30160 (32%)]\tLoss: 0.124280\n","Train Epoch: 13 [12800/30160 (42%)]\tLoss: 0.338133\n","Train Epoch: 13 [16000/30160 (53%)]\tLoss: 0.332620\n","Train Epoch: 13 [19200/30160 (64%)]\tLoss: 0.108309\n","Train Epoch: 13 [22400/30160 (74%)]\tLoss: 0.058129\n","Train Epoch: 13 [25600/30160 (85%)]\tLoss: 0.033047\n","Train Epoch: 13 [28800/30160 (96%)]\tLoss: 0.143685\n","\n","Test set: Average loss: 0.0109, Accuracy: 2396/2755 (87%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.87      0.92      2336\n","    feelings       0.55      0.85      0.66       416\n","\n","   micro avg       0.87      0.87      0.87      2752\n","   macro avg       0.76      0.86      0.79      2752\n","weighted avg       0.91      0.87      0.88      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 14 [0/30160 (0%)]\tLoss: 0.049004\n","Train Epoch: 14 [3200/30160 (11%)]\tLoss: 0.168876\n","Train Epoch: 14 [6400/30160 (21%)]\tLoss: 0.255100\n","Train Epoch: 14 [9600/30160 (32%)]\tLoss: 0.067024\n","Train Epoch: 14 [12800/30160 (42%)]\tLoss: 0.064990\n","Train Epoch: 14 [16000/30160 (53%)]\tLoss: 0.081212\n","Train Epoch: 14 [19200/30160 (64%)]\tLoss: 0.069441\n","Train Epoch: 14 [22400/30160 (74%)]\tLoss: 0.312178\n","Train Epoch: 14 [25600/30160 (85%)]\tLoss: 0.093004\n","Train Epoch: 14 [28800/30160 (96%)]\tLoss: 0.063579\n","\n","Test set: Average loss: 0.0128, Accuracy: 2360/2755 (86%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.86      0.91      2335\n","    feelings       0.52      0.85      0.64       417\n","\n","   micro avg       0.86      0.86      0.86      2752\n","   macro avg       0.74      0.85      0.78      2752\n","weighted avg       0.90      0.86      0.87      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 15 [0/30160 (0%)]\tLoss: 0.138781\n","Train Epoch: 15 [3200/30160 (11%)]\tLoss: 0.085112\n","Train Epoch: 15 [6400/30160 (21%)]\tLoss: 0.075950\n","Train Epoch: 15 [9600/30160 (32%)]\tLoss: 0.042104\n","Train Epoch: 15 [12800/30160 (42%)]\tLoss: 0.078819\n","Train Epoch: 15 [16000/30160 (53%)]\tLoss: 0.109630\n","Train Epoch: 15 [19200/30160 (64%)]\tLoss: 0.212396\n","Train Epoch: 15 [22400/30160 (74%)]\tLoss: 0.082965\n","Train Epoch: 15 [25600/30160 (85%)]\tLoss: 0.021029\n","Train Epoch: 15 [28800/30160 (96%)]\tLoss: 0.116636\n","\n","Test set: Average loss: 0.0106, Accuracy: 2447/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.90      0.93      2336\n","    feelings       0.60      0.82      0.69       416\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.86      0.81      2752\n","weighted avg       0.91      0.89      0.90      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 16 [0/30160 (0%)]\tLoss: 0.069283\n","Train Epoch: 16 [3200/30160 (11%)]\tLoss: 0.140135\n","Train Epoch: 16 [6400/30160 (21%)]\tLoss: 0.100669\n","Train Epoch: 16 [9600/30160 (32%)]\tLoss: 0.034375\n","Train Epoch: 16 [12800/30160 (42%)]\tLoss: 0.083250\n","Train Epoch: 16 [16000/30160 (53%)]\tLoss: 0.056083\n","Train Epoch: 16 [19200/30160 (64%)]\tLoss: 0.080149\n","Train Epoch: 16 [22400/30160 (74%)]\tLoss: 0.015110\n","Train Epoch: 16 [25600/30160 (85%)]\tLoss: 0.035542\n","Train Epoch: 16 [28800/30160 (96%)]\tLoss: 0.111982\n","\n","Test set: Average loss: 0.0106, Accuracy: 2445/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.90      0.93      2335\n","    feelings       0.60      0.80      0.69       417\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.85      0.81      2752\n","weighted avg       0.91      0.89      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 17 [0/30160 (0%)]\tLoss: 0.134080\n","Train Epoch: 17 [3200/30160 (11%)]\tLoss: 0.017934\n","Train Epoch: 17 [6400/30160 (21%)]\tLoss: 0.079782\n","Train Epoch: 17 [9600/30160 (32%)]\tLoss: 0.087538\n","Train Epoch: 17 [12800/30160 (42%)]\tLoss: 0.132930\n","Train Epoch: 17 [16000/30160 (53%)]\tLoss: 0.058133\n","Train Epoch: 17 [19200/30160 (64%)]\tLoss: 0.040916\n","Train Epoch: 17 [22400/30160 (74%)]\tLoss: 0.019857\n","Train Epoch: 17 [25600/30160 (85%)]\tLoss: 0.058245\n","Train Epoch: 17 [28800/30160 (96%)]\tLoss: 0.078164\n","\n","Test set: Average loss: 0.0112, Accuracy: 2440/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.90      0.93      2335\n","    feelings       0.59      0.82      0.69       417\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.86      0.81      2752\n","weighted avg       0.91      0.89      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 18 [0/30160 (0%)]\tLoss: 0.110006\n","Train Epoch: 18 [3200/30160 (11%)]\tLoss: 0.109608\n","Train Epoch: 18 [6400/30160 (21%)]\tLoss: 0.031263\n","Train Epoch: 18 [9600/30160 (32%)]\tLoss: 0.063860\n","Train Epoch: 18 [12800/30160 (42%)]\tLoss: 0.039425\n","Train Epoch: 18 [16000/30160 (53%)]\tLoss: 0.098300\n","Train Epoch: 18 [19200/30160 (64%)]\tLoss: 0.038783\n","Train Epoch: 18 [22400/30160 (74%)]\tLoss: 0.028114\n","Train Epoch: 18 [25600/30160 (85%)]\tLoss: 0.123551\n","Train Epoch: 18 [28800/30160 (96%)]\tLoss: 0.096249\n","\n","Test set: Average loss: 0.0122, Accuracy: 2423/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.89      0.93      2335\n","    feelings       0.57      0.84      0.68       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.86      0.80      2752\n","weighted avg       0.91      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 19 [0/30160 (0%)]\tLoss: 0.065332\n","Train Epoch: 19 [3200/30160 (11%)]\tLoss: 0.124921\n","Train Epoch: 19 [6400/30160 (21%)]\tLoss: 0.082448\n","Train Epoch: 19 [9600/30160 (32%)]\tLoss: 0.088021\n","Train Epoch: 19 [12800/30160 (42%)]\tLoss: 0.101132\n","Train Epoch: 19 [16000/30160 (53%)]\tLoss: 0.061799\n","Train Epoch: 19 [19200/30160 (64%)]\tLoss: 0.046690\n","Train Epoch: 19 [22400/30160 (74%)]\tLoss: 0.073747\n","Train Epoch: 19 [25600/30160 (85%)]\tLoss: 0.077733\n","Train Epoch: 19 [28800/30160 (96%)]\tLoss: 0.023014\n","\n","Test set: Average loss: 0.0108, Accuracy: 2456/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.95      0.92      0.94      2336\n","    feelings       0.62      0.75      0.68       416\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.79      0.83      0.81      2752\n","weighted avg       0.90      0.89      0.90      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 20 [0/30160 (0%)]\tLoss: 0.063585\n","Train Epoch: 20 [3200/30160 (11%)]\tLoss: 0.031435\n","Train Epoch: 20 [6400/30160 (21%)]\tLoss: 0.033196\n","Train Epoch: 20 [9600/30160 (32%)]\tLoss: 0.044061\n","Train Epoch: 20 [12800/30160 (42%)]\tLoss: 0.052237\n","Train Epoch: 20 [16000/30160 (53%)]\tLoss: 0.037705\n","Train Epoch: 20 [19200/30160 (64%)]\tLoss: 0.080691\n","Train Epoch: 20 [22400/30160 (74%)]\tLoss: 0.036356\n","Train Epoch: 20 [25600/30160 (85%)]\tLoss: 0.023603\n","Train Epoch: 20 [28800/30160 (96%)]\tLoss: 0.158420\n","\n","Test set: Average loss: 0.0143, Accuracy: 2397/2755 (87%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.88      0.92      2335\n","    feelings       0.55      0.85      0.67       417\n","\n","   micro avg       0.87      0.87      0.87      2752\n","   macro avg       0.76      0.86      0.79      2752\n","weighted avg       0.91      0.87      0.88      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 21 [0/30160 (0%)]\tLoss: 0.036212\n","Train Epoch: 21 [3200/30160 (11%)]\tLoss: 0.189846\n","Train Epoch: 21 [6400/30160 (21%)]\tLoss: 0.010760\n","Train Epoch: 21 [9600/30160 (32%)]\tLoss: 0.010073\n","Train Epoch: 21 [12800/30160 (42%)]\tLoss: 0.038287\n","Train Epoch: 21 [16000/30160 (53%)]\tLoss: 0.027634\n","Train Epoch: 21 [19200/30160 (64%)]\tLoss: 0.022064\n","Train Epoch: 21 [22400/30160 (74%)]\tLoss: 0.010963\n","Train Epoch: 21 [25600/30160 (85%)]\tLoss: 0.004218\n","Train Epoch: 21 [28800/30160 (96%)]\tLoss: 0.111991\n","\n","Test set: Average loss: 0.0124, Accuracy: 2452/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.91      0.93      2335\n","    feelings       0.61      0.81      0.69       417\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.86      0.81      2752\n","weighted avg       0.91      0.89      0.90      2752\n","\n","\n","\n","Confusion matrix, without normalization\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n","  max_open_warning, RuntimeWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 22 [0/30160 (0%)]\tLoss: 0.053270\n","Train Epoch: 22 [3200/30160 (11%)]\tLoss: 0.014875\n","Train Epoch: 22 [6400/30160 (21%)]\tLoss: 0.068824\n","Train Epoch: 22 [9600/30160 (32%)]\tLoss: 0.010809\n","Train Epoch: 22 [12800/30160 (42%)]\tLoss: 0.024205\n","Train Epoch: 22 [16000/30160 (53%)]\tLoss: 0.046728\n","Train Epoch: 22 [19200/30160 (64%)]\tLoss: 0.058011\n","Train Epoch: 22 [22400/30160 (74%)]\tLoss: 0.030302\n","Train Epoch: 22 [25600/30160 (85%)]\tLoss: 0.062198\n","Train Epoch: 22 [28800/30160 (96%)]\tLoss: 0.064958\n","\n","Test set: Average loss: 0.0148, Accuracy: 2398/2755 (87%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.88      0.92      2336\n","    feelings       0.55      0.82      0.66       416\n","\n","   micro avg       0.87      0.87      0.87      2752\n","   macro avg       0.76      0.85      0.79      2752\n","weighted avg       0.90      0.87      0.88      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 23 [0/30160 (0%)]\tLoss: 0.013788\n","Train Epoch: 23 [3200/30160 (11%)]\tLoss: 0.014709\n","Train Epoch: 23 [6400/30160 (21%)]\tLoss: 0.049001\n","Train Epoch: 23 [9600/30160 (32%)]\tLoss: 0.094859\n","Train Epoch: 23 [12800/30160 (42%)]\tLoss: 0.006331\n","Train Epoch: 23 [16000/30160 (53%)]\tLoss: 0.007709\n","Train Epoch: 23 [19200/30160 (64%)]\tLoss: 0.026389\n","Train Epoch: 23 [22400/30160 (74%)]\tLoss: 0.038089\n","Train Epoch: 23 [25600/30160 (85%)]\tLoss: 0.011581\n","Train Epoch: 23 [28800/30160 (96%)]\tLoss: 0.022537\n","\n","Test set: Average loss: 0.0133, Accuracy: 2434/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.90      0.93      2335\n","    feelings       0.59      0.79      0.68       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.85      0.80      2752\n","weighted avg       0.90      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 24 [0/30160 (0%)]\tLoss: 0.042163\n","Train Epoch: 24 [3200/30160 (11%)]\tLoss: 0.006605\n","Train Epoch: 24 [6400/30160 (21%)]\tLoss: 0.003317\n","Train Epoch: 24 [9600/30160 (32%)]\tLoss: 0.003974\n","Train Epoch: 24 [12800/30160 (42%)]\tLoss: 0.030989\n","Train Epoch: 24 [16000/30160 (53%)]\tLoss: 0.003396\n","Train Epoch: 24 [19200/30160 (64%)]\tLoss: 0.009815\n","Train Epoch: 24 [22400/30160 (74%)]\tLoss: 0.015940\n","Train Epoch: 24 [25600/30160 (85%)]\tLoss: 0.010210\n","Train Epoch: 24 [28800/30160 (96%)]\tLoss: 0.025765\n","\n","Test set: Average loss: 0.0150, Accuracy: 2414/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.89      0.92      2335\n","    feelings       0.56      0.83      0.67       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.86      0.80      2752\n","weighted avg       0.91      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 25 [0/30160 (0%)]\tLoss: 0.014559\n","Train Epoch: 25 [3200/30160 (11%)]\tLoss: 0.009301\n","Train Epoch: 25 [6400/30160 (21%)]\tLoss: 0.027918\n","Train Epoch: 25 [9600/30160 (32%)]\tLoss: 0.003186\n","Train Epoch: 25 [12800/30160 (42%)]\tLoss: 0.013568\n","Train Epoch: 25 [16000/30160 (53%)]\tLoss: 0.059194\n","Train Epoch: 25 [19200/30160 (64%)]\tLoss: 0.014762\n","Train Epoch: 25 [22400/30160 (74%)]\tLoss: 0.052628\n","Train Epoch: 25 [25600/30160 (85%)]\tLoss: 0.104873\n","Train Epoch: 25 [28800/30160 (96%)]\tLoss: 0.016663\n","\n","Test set: Average loss: 0.0152, Accuracy: 2411/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.88      0.92      2335\n","    feelings       0.56      0.84      0.67       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.76      0.86      0.80      2752\n","weighted avg       0.91      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 26 [0/30160 (0%)]\tLoss: 0.015867\n","Train Epoch: 26 [3200/30160 (11%)]\tLoss: 0.027963\n","Train Epoch: 26 [6400/30160 (21%)]\tLoss: 0.046979\n","Train Epoch: 26 [9600/30160 (32%)]\tLoss: 0.010154\n","Train Epoch: 26 [12800/30160 (42%)]\tLoss: 0.008478\n","Train Epoch: 26 [16000/30160 (53%)]\tLoss: 0.020303\n","Train Epoch: 26 [19200/30160 (64%)]\tLoss: 0.008829\n","Train Epoch: 26 [22400/30160 (74%)]\tLoss: 0.019022\n","Train Epoch: 26 [25600/30160 (85%)]\tLoss: 0.078908\n","Train Epoch: 26 [28800/30160 (96%)]\tLoss: 0.147077\n","\n","Test set: Average loss: 0.0147, Accuracy: 2423/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.90      0.93      2335\n","    feelings       0.58      0.80      0.67       417\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.85      0.80      2752\n","weighted avg       0.90      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 27 [0/30160 (0%)]\tLoss: 0.003107\n","Train Epoch: 27 [3200/30160 (11%)]\tLoss: 0.008016\n","Train Epoch: 27 [6400/30160 (21%)]\tLoss: 0.008302\n","Train Epoch: 27 [9600/30160 (32%)]\tLoss: 0.012707\n","Train Epoch: 27 [12800/30160 (42%)]\tLoss: 0.009334\n","Train Epoch: 27 [16000/30160 (53%)]\tLoss: 0.022172\n","Train Epoch: 27 [19200/30160 (64%)]\tLoss: 0.014987\n","Train Epoch: 27 [22400/30160 (74%)]\tLoss: 0.075383\n","Train Epoch: 27 [25600/30160 (85%)]\tLoss: 0.082292\n","Train Epoch: 27 [28800/30160 (96%)]\tLoss: 0.071685\n","\n","Test set: Average loss: 0.0169, Accuracy: 2367/2755 (86%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.87      0.91      2335\n","    feelings       0.52      0.83      0.64       417\n","\n","   micro avg       0.86      0.86      0.86      2752\n","   macro avg       0.74      0.85      0.78      2752\n","weighted avg       0.90      0.86      0.87      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 28 [0/30160 (0%)]\tLoss: 0.036350\n","Train Epoch: 28 [3200/30160 (11%)]\tLoss: 0.038083\n","Train Epoch: 28 [6400/30160 (21%)]\tLoss: 0.050826\n","Train Epoch: 28 [9600/30160 (32%)]\tLoss: 0.006705\n","Train Epoch: 28 [12800/30160 (42%)]\tLoss: 0.021627\n","Train Epoch: 28 [16000/30160 (53%)]\tLoss: 0.004055\n","Train Epoch: 28 [19200/30160 (64%)]\tLoss: 0.016529\n","Train Epoch: 28 [22400/30160 (74%)]\tLoss: 0.051929\n","Train Epoch: 28 [25600/30160 (85%)]\tLoss: 0.031612\n","Train Epoch: 28 [28800/30160 (96%)]\tLoss: 0.008503\n","\n","Test set: Average loss: 0.0138, Accuracy: 2443/2755 (89%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.91      0.93      2335\n","    feelings       0.60      0.76      0.67       417\n","\n","   micro avg       0.89      0.89      0.89      2752\n","   macro avg       0.78      0.84      0.80      2752\n","weighted avg       0.90      0.89      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 29 [0/30160 (0%)]\tLoss: 0.007694\n","Train Epoch: 29 [3200/30160 (11%)]\tLoss: 0.090721\n","Train Epoch: 29 [6400/30160 (21%)]\tLoss: 0.010420\n","Train Epoch: 29 [9600/30160 (32%)]\tLoss: 0.015041\n","Train Epoch: 29 [12800/30160 (42%)]\tLoss: 0.190334\n","Train Epoch: 29 [16000/30160 (53%)]\tLoss: 0.017579\n","Train Epoch: 29 [19200/30160 (64%)]\tLoss: 0.012279\n","Train Epoch: 29 [22400/30160 (74%)]\tLoss: 0.016746\n","Train Epoch: 29 [25600/30160 (85%)]\tLoss: 0.144139\n","Train Epoch: 29 [28800/30160 (96%)]\tLoss: 0.241099\n","\n","Test set: Average loss: 0.0155, Accuracy: 2421/2755 (88%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.96      0.89      0.93      2336\n","    feelings       0.57      0.80      0.67       416\n","\n","   micro avg       0.88      0.88      0.88      2752\n","   macro avg       0.77      0.85      0.80      2752\n","weighted avg       0.90      0.88      0.89      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 30 [0/30160 (0%)]\tLoss: 0.007035\n","Train Epoch: 30 [3200/30160 (11%)]\tLoss: 0.009056\n","Train Epoch: 30 [6400/30160 (21%)]\tLoss: 0.009699\n","Train Epoch: 30 [9600/30160 (32%)]\tLoss: 0.002559\n","Train Epoch: 30 [12800/30160 (42%)]\tLoss: 0.032474\n","Train Epoch: 30 [16000/30160 (53%)]\tLoss: 0.021355\n","Train Epoch: 30 [19200/30160 (64%)]\tLoss: 0.025080\n","Train Epoch: 30 [22400/30160 (74%)]\tLoss: 0.030162\n","Train Epoch: 30 [25600/30160 (85%)]\tLoss: 0.005245\n","Train Epoch: 30 [28800/30160 (96%)]\tLoss: 0.005950\n","\n","Test set: Average loss: 0.0194, Accuracy: 2364/2755 (86%)\n","\n","              precision    recall  f1-score   support\n","\n","      others       0.97      0.86      0.91      2335\n","    feelings       0.52      0.83      0.64       417\n","\n","   micro avg       0.86      0.86      0.86      2752\n","   macro avg       0.74      0.85      0.78      2752\n","weighted avg       0.90      0.86      0.87      2752\n","\n","\n","\n","Confusion matrix, without normalization\n","Train Epoch: 31 [0/30160 (0%)]\tLoss: 0.015062\n","Train Epoch: 31 [3200/30160 (11%)]\tLoss: 0.007423\n"],"name":"stdout"}]},{"metadata":{"id":"45NPYB6GFcyP","colab_type":"code","colab":{}},"cell_type":"code","source":["for batch_idx,(data,target) in enumerate(data_loader_train):\n","  print(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pAqB4F8eFmuo","colab_type":"code","colab":{}},"cell_type":"code","source":["for batch_idx,(data,target) in enumerate(data_loader_test):\n","  print(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"537DWjIKYOT3","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","x = torch.LongTensor([[[3, 4, 1], [8, 1, 6]], [[7,2,3], [5,3,8]]])\n","print(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EHdiIsSIYgpc","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.max(x, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j1vPpseLZLRD","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}