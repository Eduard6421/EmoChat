{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BidirectionalWordsNCharsFullRNNMaxHidden.ipynb","version":"0.3.2","provenance":[{"file_id":"1XeYatveZlAqFUYUG3cTIx2zhbTpZ4TAT","timestamp":1555343513787},{"file_id":"1BC8eJ8arIEy7d7_t2hxk3wk5rCBoFFL2","timestamp":1555172354450},{"file_id":"1LPiZl2F6SRNUQ3VDyHR91mSwMoJW7h-9","timestamp":1555161651655},{"file_id":"1MwJSWKyMObQnQscRIRa0YJn5urCEbkRR","timestamp":1555156242884},{"file_id":"1MwJSWKyMObQnQscRIRa0YJn5urCEbkRR","timestamp":1555152441220},{"file_id":"1ZIUZC-sPimWy1RSqtNoNEG7XX_yqNi71","timestamp":1554971018806}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WETg5r8EhbVo","colab_type":"code","outputId":"70f70595-0a28-46e4-cafb-76f6f80b0331","executionInfo":{"status":"ok","timestamp":1555947977254,"user_tz":-180,"elapsed":3901,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["import json\n","import torch\n","import re\n","import numpy as np\n","import collections\n","\n","from torch import nn\n","from torch.optim import SGD, Adam\n","from google.colab import files, auth, drive\n","from urllib.request import urlopen\n","from typing import List, Dict, Callable\n","from collections import Counter\n","from os import path\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","drive.mount('/content/gdrive',force_remount=True)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","cuda\n"],"name":"stdout"}]},{"metadata":{"id":"TStmc8ibe73Z","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  HYPERPARAMETERS  ================================='''\n","\n","_hyperparameters_dict = {\n","    \"batch_size\": 32,\n","    \"num_epochs\": 5,\n","    \"max_len\": 200,\n","    \"max_sentence_word_length\" : 30,\n","    \"embedding_size\": 100,\n","    \"rnn_size\": 512,\n","    \"learning_algo\": \"adam\",\n","    \"learning_rate\": 0.0001,\n","    \"output_size\" : 4,\n","    \"max_grad_norm\": 3.0\n","}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QH0UNqvjmh6C","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  DEFINES  ================================='''\n","\n","'''======= Paths ======='''\n","\n","TrainPath = \"gdrive/My Drive/Datasets/Emoji/demojized_train.txt\"\n","DevPath = \"gdrive/My Drive/Datasets/Emoji/demojized_dev.txt\"\n","TestPath = \"gdrive/My Drive/Datasets/Emoji/demojized_test.txt\"\n","FunPath = \"gdrive/My Drive/Datasets/Emoji/newfile1.txt\"\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mZrvGikXFNB4","colab_type":"code","outputId":"44e68707-5ece-46d9-caf2-4a1a0d73f240","executionInfo":{"status":"ok","timestamp":1555940465163,"user_tz":-180,"elapsed":7114,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"cell_type":"code","source":["%ls \"gdrive/My Drive/Datasets/Emoji/\""],"execution_count":5,"outputs":[{"output_type":"stream","text":["demojized_devEmotion.txt    demojized_trainEmotion.txt\n","demojized_dev_feelings.txt  demojized_train_feelings.txt\n","demojized_dev.txt           demojized_train.txt\n","demojized_test.txt          glove.twitter.27B.100d.txt\n"],"name":"stdout"}]},{"metadata":{"id":"-2oXk7VZSA-4","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  EXTERNAL FUNCTIONS  ================================='''\n","\n","\n","def readfile(filepath: str) -> str:\n","    \"\"\"\n","    Reads file and returns its content as a string.\n","    \"\"\"\n","    #response = urlopen(url)\n","    #body = response.read().decode('utf-8')\n","    with open(filepath, \"r\") as f:\n","        body = f.read()\n","    \n","    return body.encode('ascii', 'ignore').decode(\"utf-8\")\n","\n","  \n","def find_all(a_str, sub):\n","    start = 0\n","    while True:\n","        start = a_str.find(sub, start)\n","        if start == -1: return\n","        yield start\n","        start += len(sub) \n","  \n","  \n","  \n","\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    #print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7EmcK3GrlcPj","colab_type":"code","colab":{}},"cell_type":"code","source":["'''================================= CUSTOM DATA-STRUCTURES  ================================='''\n","\n","class OneVsOthersDict():\n","  \n","  def __init__(self,):\n","      self.to_number = collections.OrderedDict()\n","      self.to_language = collections.OrderedDict()\n","    \n","  def insert(self,word):    \n","    if word == 'others':\n","      self.to_number[word] = 0\n","      self.to_language[0] = word\n","    else:\n","      self.to_number[word] = 1\n","      self.to_language[1] = 'feelings'\n","        \n","        \n","  def printitems(self):\n","      for label,value in self.to_number.items():\n","        print(label + ' ' + str(value))\n","        \n","  def getindex(self,word):\n","    return self.to_number[word]\n","  \n","  def getlabel(self,label):\n","    return self.to_language[label]\n","  \n","  def getlabellist(self):\n","    label_list = []\n","    for label,value in self.to_language.items():\n","      label_list.append(value)\n","    \n","    return label_list\n","  \n","\n","class TwoWayDict():\n","  \n","  def __init__(self,):\n","    self.to_number = collections.OrderedDict()\n","    self.to_language = collections.OrderedDict()\n","    \n","\n","    \n","  def insert(self,word):\n","    if not word in self.to_number:\n","      new_index = len(self.to_number)\n","      self.to_number[word] = new_index\n","      self.to_language[new_index] = word\n","    \n","    \n","  def getindex(self,word):\n","    return self.to_number[word]\n","  \n","  def getlabel(self,label):\n","    return self.to_language[label]\n","  \n","  def getlabellist(self):\n","    label_list = []\n","    for label,value in self.to_language.items():\n","      label_list.append(value)\n","    \n","    return label_list\n","\n"," \n","  \n","  \n","class Vocabulary:\n","    \"\"\"\n","    Helper class that maps characters to unique indices and the other way around\n","    \"\"\"\n","    def __init__(self, text: str):\n","        #special character for padding shorter sequences in a mini-batches\n","        characters_set = set(\"Â©\") \n","        characters_set.update(text)\n","\n","        \n","        self.char_to_idx = {char:idx for (idx, char) \n","                            in enumerate(characters_set)}\n","        \n","       \n","        self.idx_to_char = {idx:char for (idx, char) \n","                            in enumerate(characters_set)}\n","   \n","    def size(self):\n","        return len(self.char_to_idx)\n","      \n","    def __str__(self):\n","        return str(self.char_to_idx)\n","  \n","  \n","def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n","    \"\"\"\n","    Convert a string to a Tensor with corresponding character indices\n","    e.g. \"We have\" -> [12, 6, 20, 13, 1, 25, 6] \n","    \"\"\"\n","    \n","    \n","    text_indices = [vocab.char_to_idx[c] for c in text]\n","    \n","    return torch.tensor(text_indices)  \n","\n","  \n","def tensor_to_text(x: torch.LongTensor, vocab: Vocabulary) -> str:\n","    \"\"\"\n","    Convert a Tensor of character indices to its string representation\n","    e.g. [12, 6, 20, 13, 1, 25, 6] -> \"We have\"\n","    \"\"\"\n","    return \"\".join(vocab.idx_to_char[idx.item()] for idx in x) \n","  \n","  \n","def get_vocabulary(trainPath : str):\n","    \n","    text = readfile(trainPath)\n","    \n","    vocab = Vocabulary(text)\n","    \n","    return vocab\n","\n","  \n","class WordVocab:\n","  \n","  \n","    def __init__(self, text: str):\n","        #special character for padding shorter sequences in a mini-batches\n","        \n","        characters_set = {'<unk>':0,'<pad>':1}\n"," \n","        cnt = 2\n","        for word in text:\n","            if not word in characters_set:\n","              characters_set[word] = cnt\n","              cnt +=1\n","\n","        \n","        self.char_to_idx = characters_set\n","       \n","        self.idx_to_char = {idx:char for (idx, char) \n","                            in enumerate(characters_set)}\n","        \n","        #or idx in self.idx_to_char:\n","        #  print(self.idx_to_char[idx])\n","  \n","    def size(self):\n","        return len(self.char_to_idx)\n","      \n","    def __str__(self):\n","        return str(self.char_to_idx)\n","  \n","    \n","    def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n","    \n","      text_indices = [vocab.char_to_idx[c] for c in text]\n","    \n","      return torch.tensor(text_indices)  \n","\n","def text_to_tensor_word(x, wordvocab,max_length):\n","  \n","  \n","    list = []\n","  \n","    for word in x:\n","      if word in wordvocab.char_to_idx:\n","        list.append(wordvocab.char_to_idx[word])\n","      else:\n","        list.append(wordvocab.char_to_idx['<unk>'])\n","    \n","    \n","    while(len(list) > max_length):\n","          list.pop()\n","    \n","    \n","    while(len(list) < max_length):\n","      list.append(wordvocab.char_to_idx['<pad>'])\n","    \n","    \n","    return torch.tensor(list)\n","    \n","  \n","def get_wordvocab(dataPath):\n","    text = readfile(dataPath)\n","      \n","    vocab = WordVocab(text.split())\n","       \n","      \n","    return vocab\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"pwMshf8hhnY3","colab_type":"code","outputId":"b7b9d987-0587-4000-b2df-c6c0234795e5","executionInfo":{"status":"ok","timestamp":1555940465980,"user_tz":-180,"elapsed":7911,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["'''=================================   GLOVE EMBEDDING  ================================='''\n","\n","'''====== Load Glove Model======'''\n","\n","def loadGloveModel(gloveFile):\n","    print(\"Loading Glove Model\")\n","    f = open(gloveFile,'r')\n","    model = {}\n","    for line in f:\n","        splitLine = line.split()\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print(\"Done.\",len(model),\" words loaded!\")\n","    return model\n","\n","  \n","'''====== Get Word Embeddings ======'''\n","\n","def get_value(word,gloveModel):\n","  answer = torch.zeros(100,dtype=torch.float64)  \n","  if word in gloveModel.keys():\n","    answer = answer + torch.tensor(gloveModel[word],dtype=torch.float64)  \n","  return answer\n","\n","\n","def get_tensor_form(phrase,gloveModel):  \n","  \n","  phrase_tensor = get_value(phrase[0],gloveModel)\n","  for i in range(1,len(phrase)):\n","    phrase_tensor += get_value(phrase[i], gloveModel)\n","\n","  return phrase_tensor  \n","  \n","  \n","  \n","'''====== Instantiate Glove Model ======'''\n","  \n","  \n","#glovePath = \"gdrive/My Drive/Datasets/Emoji/glove.twitter.27B.100d.txt\"\n","#gloveModel = loadGloveModel(glovePath)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'====== Instantiate Glove Model ======'"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"6Hgm8lUEyCgm","colab_type":"code","colab":{}},"cell_type":"code","source":["def cut_phrase(phrase, max_length):\n","\n","  \n","  phrase_bounds = list(find_all(phrase, '<end>'))\n","\n","  leftsentence = \"\"\n","  middlesentence = \"\"\n","  rightsentence = \"\"\n","        \n","  for i in range(phrase_bounds[0],phrase_bounds[1]):\n","    leftsentence = leftsentence + phrase[i]\n","  \n","  for i in range(phrase_bounds[1],phrase_bounds[2]):\n","    middlesentence = middlesentence + phrase[i]\n","\n","  for i in range(phrase_bounds[2],phrase_bounds[3]):\n","    rightsentence = rightsentence + phrase[i]\n","\n","  \n","  l1 = len(leftsentence)\n","  l2 = len(middlesentence)\n","  l3 = len(rightsentence)\n","  \n","  '''\n","  -8 deoarece de la ultima se taie fix <end> si vrem sa adaugam si un spatiu\n","  de asemenea se modifica cea din mijloc si la stanga si la dreapta\n","  '''\n","  \n","  while(l1+l2+l3 > max_length - 8):\n","    \n","    if(l1 >= l2 and l1 >= l3):\n","      leftsentence = leftsentence[:-1]\n","      l1 -= 1\n","    elif(l2 >= l1 and l2 >= l3):\n","      middlesentence = middlesentence[:-1]\n","      l2 -= 1\n","    else:\n","      rightsentence = rightsentence[:-1]\n","      l3 -= 1\n","    \n","  middlesentence = \" \" + middlesentence + \" \"\n","  rightsentence = rightsentence + \" <end>\"\n","  \n","  #print(leftsentence + middlesentence + rightsentence  )\n","    \n","  return leftsentence + middlesentence + rightsentence  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_MpcBPm5m69f","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================   DATA LOADING  ================================='''\n","def get_data(path : str,  label_dict: TwoWayDict):\n","  print(path)\n","  labelcounter = 0\n","  words = []\n","  labels = []\n","  with open(path) as file:\n","    cnt = 0 \n","    for line in file:\n","      if cnt > 0:\n","        label = line.split()[-1]\n","        \n","        labels.append(label)\n","        \n","        templine = line.split(' ')[1:-1]\n","        \n","        words.append(templine)\n","\n","        label_dict.insert(label)\n","\n","      cnt+=1\n","  \n","  return [words,labels]\n","  \n","  \n","  \n","def get_dataset_loader(words, labels, label_dict, vocabulary, word_vocabulary,max_length=95, max_words=50, max_batch_idx=100, shuffle=True):\n","  \n","  dataset_length = len(labels)\n","\n","  traintensor = torch.zeros(dataset_length,max_length + max_words + 3) # Cele 3 hidden state-uri pe care le cautam :)\n","  labeltensor = torch.zeros(dataset_length,1)\n","  \n","  maxwords = 0\n","  \n","  padding_tensor = torch.tensor([vocabulary.char_to_idx[\"Â©\"]])\n","  \n","  for i in range(0, dataset_length):\n","\n","    \n","    phrase = words[i]\n","\n","    currentlabel = labels[i]  \n","    phrase = ' '.join(phrase)\n","    \n","    word_phrase = text_to_tensor_word(phrase, word_vocabulary,max_words)\n","    \n","    \n","    if(len(phrase) > max_length):\n","      phrase = cut_phrase(phrase,max_length)\n","    \n","    hidden_state_positions = []\n","    \n","    \n","    hidden_state_positions = list(find_all(phrase, '<end>'))\n","    hidden_state_positions = [x+4 for x in hidden_state_positions]\n","    hidden_state_positions.pop(0) # delete the first <end>\n","    \n","    \n","    #print(hidden_state_positions)\n","    \n","    #print(phrase)\n","\n","    \n","    phrase = text_to_tensor(phrase,vocabulary)\n","    \n","    while(phrase.size()[0] < max_length):\n","      phrase = torch.cat((phrase,padding_tensor))\n","     \n","    \n","    currentlabel = label_dict.getindex(currentlabel)\n","\n","\n","    hidden_state_positions = torch.tensor(hidden_state_positions).type(torch.LongTensor)\n","    #print(hidden_state_positions)\n","    \n","    #print(hidden_state_positions)\n","    \n","    phrase = torch.cat((phrase,hidden_state_positions))\n","    phrase = torch.cat((phrase,word_phrase))\n","    phrase = phrase.to(device)\n","    \n","    #print(phrase.size())\n","    \n","    newlabel = torch.zeros(1).to(device).long()\n","    newlabel[0]=currentlabel\n","    currentlabel = newlabel\n","\n","    traintensor[i] = phrase\n","    labeltensor[i] = currentlabel\n","    \n","  \n","  large_dataset = TensorDataset(torch.tensor(traintensor).to(device), torch.tensor(labeltensor).to(device))\n","  large_data_loader = DataLoader(large_dataset, batch_size=max_batch_idx, shuffle=True, drop_last=True)\n","  return large_data_loader"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"0608c706-f407-4d30-bd7b-4370a6e6dbd0","executionInfo":{"status":"ok","timestamp":1555940486979,"user_tz":-180,"elapsed":28891,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"id":"5ZGbztuprlAD","colab":{"base_uri":"https://localhost:8080/","height":104}},"cell_type":"code","source":["label_dict = TwoWayDict()\n","vocab = get_vocabulary(TrainPath)\n","wordvocab = get_wordvocab(TrainPath)\n","\n","\n","[trainingwords,traininglabels] = get_data(TrainPath,label_dict)\n","data_loader_train = get_dataset_loader(trainingwords,traininglabels,label_dict, vocab, wordvocab, _hyperparameters_dict['max_len'],_hyperparameters_dict['max_sentence_word_length'], _hyperparameters_dict['batch_size'])\n","\n","[testingwords,testinglabels] = get_data(TestPath,label_dict)\n","data_loader_test  = get_dataset_loader(testingwords,testinglabels,label_dict, vocab, wordvocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['max_sentence_word_length'],_hyperparameters_dict['batch_size'])\n","\n","[devwords,devlabels] = get_data(DevPath,label_dict)\n","data_loader_dev  = get_dataset_loader(devwords,devlabels,label_dict, vocab, wordvocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['max_sentence_word_length'],_hyperparameters_dict['batch_size'])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["gdrive/My Drive/Datasets/Emoji/demojized_train.txt\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["gdrive/My Drive/Datasets/Emoji/demojized_test.txt\n","gdrive/My Drive/Datasets/Emoji/demojized_dev.txt\n"],"name":"stdout"}]},{"metadata":{"id":"6VzxiDqFgLOX","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_data_fun(sentence):\n","  words = []\n","  labels = []\n","  label = sentence.split()[-1]\n","  labels.append(label)\n","  templine = sentence.split(' ')[1:-1]\n","  words.append(templine)\n","  label_dict.insert(label)\n","  \n","  return [words,labels]\n","\n","\n","\n","def get_dataset_loader(words, labels, label_dict, vocabulary, word_vocabulary,max_length=95, max_words=50, max_batch_idx=100, shuffle=True):\n","  \n","  dataset_length = len(labels)\n","\n","  traintensor = torch.zeros(dataset_length,max_length + max_words + 3) # Cele 3 hidden state-uri pe care le cautam :)\n","  labeltensor = torch.zeros(dataset_length,1)\n","  \n","  maxwords = 0\n","  \n","  padding_tensor = torch.tensor([vocabulary.char_to_idx[\"Â©\"]])\n","  \n","  for i in range(0, dataset_length):\n","\n","    \n","    phrase = words[i]\n","\n","    currentlabel = labels[i]  \n","    phrase = ' '.join(phrase)\n","    \n","    word_phrase = text_to_tensor_word(phrase, word_vocabulary,max_words)\n","    \n","    \n","    if(len(phrase) > max_length):\n","      phrase = cut_phrase(phrase,max_length)\n","    \n","    hidden_state_positions = []\n","    \n","    \n","    hidden_state_positions = list(find_all(phrase, '<end>'))\n","    hidden_state_positions = [x+4 for x in hidden_state_positions]\n","    hidden_state_positions.pop(0) # delete the first <end>\n","    \n","    \n","    #print(hidden_state_positions)\n","    \n","    #print(phrase)\n","\n","    \n","    phrase = text_to_tensor(phrase,vocabulary)\n","    \n","    while(phrase.size()[0] < max_length):\n","      phrase = torch.cat((phrase,padding_tensor))\n","     \n","    \n","    currentlabel = label_dict.getindex(currentlabel)\n","\n","\n","    hidden_state_positions = torch.tensor(hidden_state_positions).type(torch.LongTensor)\n","    #print(hidden_state_positions)\n","    \n","    #print(hidden_state_positions)\n","    \n","    phrase = torch.cat((phrase,hidden_state_positions))\n","    phrase = torch.cat((phrase,word_phrase))\n","    phrase = phrase.to(device)\n","    \n","    #print(phrase.size())\n","    \n","    newlabel = torch.zeros(1).to(device).long()\n","    newlabel[0]=currentlabel\n","    currentlabel = newlabel\n","\n","    traintensor[i] = phrase\n","    labeltensor[i] = currentlabel\n","    \n","  \n","  large_dataset = TensorDataset(torch.tensor(traintensor).to(device), torch.tensor(labeltensor).to(device))\n","  large_data_loader = DataLoader(large_dataset, batch_size=max_batch_idx, shuffle=True, drop_last=True)\n","  return large_data_loader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KFnaYiWyrrGH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"6e6f3144-d61f-4dea-cf62-346c78aabfe8","executionInfo":{"status":"ok","timestamp":1555947257844,"user_tz":-180,"elapsed":3757,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}}},"cell_type":"code","source":["% ls \"gdrive/My Drive/Datasets/Emoji/\""],"execution_count":55,"outputs":[{"output_type":"stream","text":["demojized_devEmotion.txt    demojized_trainEmotion.txt    newfile1.txt\n","demojized_dev_feelings.txt  demojized_train_feelings.txt  newfile.gdoc\n","demojized_dev.txt           demojized_train.txt           newfile.txt\n","demojized_test.txt          glove.twitter.27B.100d.txt\n"],"name":"stdout"}]},{"metadata":{"id":"Byfo82N5eKfa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"f14c7feb-8b7f-4d40-8f32-efdd222bc598","executionInfo":{"status":"ok","timestamp":1555947264962,"user_tz":-180,"elapsed":1004,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}}},"cell_type":"code","source":["% ls \"gdrive/My Drive/Datasets/Emoji/\"\n","FunPath = \"gdrive/My Drive/Datasets/Emoji/newfile1.txt\"\n","[funwords,funlabels] = get_data(FunPath,label_dict)\n","print(label_dict.getlabellist())\n","data_loader_fun  = get_dataset_loader(funwords,funlabels,label_dict, vocab, wordvocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['max_sentence_word_length'],_hyperparameters_dict['batch_size'])"],"execution_count":56,"outputs":[{"output_type":"stream","text":["gdrive/My Drive/Datasets/Emoji/newfile1.txt\n","['others', 'angry', 'sad', 'happy']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"}]},{"metadata":{"id":"hrRg2m3qG1je","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"_1cawLJh3ACH","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  NETWORK DEFINITION  ================================='''\n","\n","class RNNLM(nn.Module):\n","    def __init__(self, vocab_size: int,word_vocab_size:int, char_embedding_size: int,\n","                 rnn_size: int, final_output_size:int):\n","        super().__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.char_embedding_size = char_embedding_size\n","        self.rnn_size = rnn_size\n","\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                      embedding_dim=char_embedding_size)\n","        \n","        \n","        self.embedding_words = nn.Embedding(num_embeddings=word_vocab_size,\n","                                      embedding_dim=char_embedding_size)\n","\n","        self.rnn_cell = nn.LSTMCell(input_size=char_embedding_size,\n","                                   hidden_size=rnn_size)\n","        \n","        self.rnn_cell_reversed = nn.LSTMCell(input_size=char_embedding_size,\n","                                   hidden_size=rnn_size)\n","                \n","        self.rnn_cell_words = nn.LSTMCell(input_size=char_embedding_size,\n","                                   hidden_size=rnn_size)\n","        \n","        self.rnn_cell_words_reversed = nn.LSTMCell(input_size=char_embedding_size,\n","                                   hidden_size=rnn_size)\n","        \n","        self.logits = nn.Linear(in_features=rnn_size*4, out_features=100)\n","        self.newfcn = nn.Linear(in_features=100,out_features=final_output_size)\n","        self.dropout = nn.Dropout(p=0.3, inplace=True)\n","        self.relu = nn.ReLU()\n","        \n","        self.loss = nn.CrossEntropyLoss()\n","        \n","\n","    def get_loss(self, logits: torch.FloatTensor, y: torch.FloatTensor):\n","\n","        y = y.view(-1)\n","      \n","      \n","        #print(logits.size())\n","      \n","        return self.loss(logits, y)\n","\n","    def get_logits(self, hidden_states: torch.FloatTensor,hidden_states_words : torch.FloatTensor,hidden_states_reversed: torch.FloatTensor,hidden_states_words_reversed: torch.FloatTensor,\n","                   temperature: float = 1):\n","      \n","        max_len = hidden_states.size(1)\n","        \n","        hidden_states = torch.cat((hidden_states,hidden_states_reversed),dim=1)\n","        hidden_states_words = torch.cat((hidden_states_words,hidden_states_words_reversed),dim=1)\n","        \n","        hidden_state_max = torch.max(hidden_states,dim=1)[0]\n","        hidden_state_max_words = torch.max(hidden_states_words,dim=1)[0]\n","        \n","        \n","        product_hidden = hidden_state_max * hidden_state_max_words\n","        substraction_hidden = hidden_state_max - hidden_state_max_words\n","        \n","        \n","        \n","        \n","        answer = torch.cat((hidden_state_max.type(torch.FloatTensor),hidden_state_max_words.type(torch.FloatTensor),product_hidden.type(torch.FloatTensor),substraction_hidden.type(torch.FloatTensor)),dim=1).to(device)\n","\n","        answer = self.relu(self.logits(answer))\n","        answer = self.dropout(answer)\n","        \n","        return self.newfcn(answer)         \n","        \n","\n","    def forward(self, x: torch.LongTensor,\n","                hidden_start: torch.FloatTensor = None, cell_start : torch.FloatTensor = None,\n","                hidden_start_words: torch.FloatTensor = None, cell_start_words : torch.FloatTensor = None,\n","                hidden_start_reversed: torch.FloatTensor = None, cell_start_reversed : torch.FloatTensor = None,\n","                hidden_start_words_reversed: torch.FloatTensor = None, cell_start_words_reversed : torch.FloatTensor = None) -> torch.FloatTensor:\n","\n","\n","        max_len = _hyperparameters_dict[\"max_len\"]\n","        max_len_words =_hyperparameters_dict[\"max_sentence_word_length\"]\n","        \n","        # de la 0 la hyper_dict.max_len-1 sunt tensorii ptr caractere\n","        # de la dict.max_len la dict.max_len+3 sunt token-urile <end>\n","        # de la dict.max_len+4 la final sunt tensorii pentru cuvinte\n","\n","        \n","        x1_embedded = self.embedding(x[:,0:_hyperparameters_dict['max_len']])\n","        x1_embedded_words = self.embedding_words(x[:,_hyperparameters_dict['max_len']+3:x.size()[1]])\n","\n","        hidden_states_list = []\n","        prev_hidden = hidden_start\n","        prev_cell = cell_start      \n","        \n","        for t in range(max_len):\n","\n","            hidden_state,hidden_cell = self.rnn_cell(x1_embedded[:, t, :], (prev_hidden,prev_cell))\n","            hidden_states_list.append(hidden_state)\n","\n","            prev_hidden = hidden_state\n","            prev_cell = hidden_cell\n","\n","        hidden_states = torch.stack(hidden_states_list, dim=1)\n","        \n","        \n","        hidden_states_list_reversed = []\n","        prev_hidden_reversed = hidden_start_reversed\n","        prev_cell_reversed = cell_start_reversed      \n","        \n","        for t in range(max_len):\n","\n","            hidden_state_reversed,hidden_cell_reversed = self.rnn_cell_reversed(x1_embedded[:, max_len-t-1, :], (prev_hidden_reversed,prev_cell_reversed))\n","            hidden_states_list_reversed.append(hidden_state_reversed)\n","\n","            prev_hidden_reversed = hidden_state_reversed\n","            prev_cell_reversed = hidden_cell_reversed\n","\n","        hidden_states_reversed = torch.stack(hidden_states_list_reversed, dim=1)\n","        \n","        \n","        hidden_states_list_words = []\n","        prev_hidden_words = hidden_start_words\n","        prev_cell_words = cell_start_words\n","        \n","        for t in range(max_len_words):\n","            hidden_state_words,hidden_cell_words = self.rnn_cell_words(x1_embedded_words[:, t, :], (prev_hidden_words,prev_cell_words))\n","            hidden_states_list_words.append(hidden_state_words)\n","\n","            prev_hidden_words = hidden_state_words\n","            prev_cell_words = hidden_cell_words\n","            \n","        hidden_states_words = torch.stack(hidden_states_list_words, dim=1)\n","        \n","        \n","        hidden_states_list_words_reversed = []\n","        prev_hidden_words_reversed = hidden_start_words_reversed\n","        prev_cell_words_reversed = cell_start_words_reversed\n","        \n","        for t in range(max_len_words):\n","            hidden_state_words_reversed,hidden_cell_words_reversed = self.rnn_cell_words_reversed(x1_embedded_words[:, max_len_words-t-1, :], (prev_hidden_words_reversed,prev_cell_words_reversed))\n","            hidden_states_list_words_reversed.append(hidden_state_words_reversed)\n","\n","            prev_hidden_words_reversed = hidden_state_words_reversed\n","            prev_cell_words_reversed = hidden_cell_words_reversed\n","            \n","        hidden_states_words_reversed = torch.stack(hidden_states_list_words_reversed, dim=1)\n","        \n","        \n","        \n","        \n","        return [hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D1LkG34akpJB","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"mALNHpahyhD5","colab_type":"code","colab":{}},"cell_type":"code","source":["'''=================================  RUN THE TRAINING / TESTING  ================================='''\n","\n","def trainRNNM(model, device, train_loader, optimizer, epoch, verbose):\n","    \n","    model.train()\n","    \n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","\n","    prev_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    train_epoch_loss = 0\n","    correct = 0\n","    ground_truth_array = []\n","    prediction_array = []\n","    \n","    \n","    \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","      \n","        hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","        \n","        data = data.to(device)\n","      \n","        data = data.type(torch.LongTensor).to(device)\n","        target = target.type(torch.LongTensor).to(device)\n","        hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","        \n","        optimizer.zero_grad()\n","\n","        [hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed] = model(data, prev_hidden, cell_hidden, prev_hidden_words, cell_hidden_words,prev_hidden_reversed,cell_hidden_reversed,prev_hidden_words_reversed,cell_hidden_words_reversed)        \n","        logits = model.get_logits(hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed)        \n","        loss = model.get_loss(logits,target)\n","        train_epoch_loss += loss.item()\n","        pred = logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        \n","        \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(list(model.parameters()),_hyperparameters_dict[\"max_grad_norm\"])\n","\n","        optimizer.step()\n","\n","        hidden_states.detach()\n","        hidden_states_words.detach()\n","        \n","        \n","        \n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","        prediction_array.extend(pred.view(-1)[:].cpu().numpy())\n","        ground_truth_array.extend(target[:].cpu().numpy())\n","        \n","        \n","\n","        if batch_idx % verbose == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                       100. * batch_idx / len(train_loader), loss.item()))\n","            # plot_loss(loss.cpu().detach().numpy(),label='train')\n","            \n","    train_epoch_loss /= len(train_loader.dataset)\n","    f1_scores = f1_score(ground_truth_array, prediction_array, average=None)\n","    current_accuracy = accuracy_score(ground_truth_array, prediction_array)\n","\n","    return [f1_scores,current_accuracy,train_epoch_loss]\n","\n","  \n","  \n","def devRNNM(model, device, test_loader,verbose):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    \n","    ground_truth_array = []\n","    prediction_array = []\n","    \n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","\n","    prev_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    with torch.no_grad():\n","        for batch_idx,(data, target) in enumerate(test_loader):\n","            \n","            hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","            data = data.type(torch.LongTensor).to(device)\n","            target = target.type(torch.LongTensor).to(device)\n","            hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","\n","            [hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed] = model(data, prev_hidden, cell_hidden, prev_hidden_words, cell_hidden_words,prev_hidden_reversed,cell_hidden_reversed,prev_hidden_words_reversed,cell_hidden_words_reversed)        \n","            logits = model.get_logits(hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed)          \n","            loss = model.get_loss(logits,target)\n","\n","\n","\n","            hidden_states.detach()\n","\n","            test_loss += loss.item()\n","            pred = logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            \n","            \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","            prediction_array.extend(pred.view(-1)[:].cpu().numpy())\n","            ground_truth_array.extend(target[:].cpu().numpy())\n","          \n","            \n","\n","   \n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    #plot_loss(test_loss,label='test',color='red')\n","    \n","    \n","    f1_scores = f1_score(ground_truth_array, prediction_array, average=None)\n","    current_accuracy = accuracy_score(ground_truth_array, prediction_array)\n","    \n","        \n","    print(classification_report(ground_truth_array, prediction_array, target_names=label_dict.getlabellist()))\n","    \n","    \n","    print('\\n')\n","                                          \n","                                      \n","    return [f1_scores,current_accuracy,test_loss]  \n","  \n","\n","            \n","def testRNNM(model, device, test_loader,verbose):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    \n","    ground_truth_array = []\n","    prediction_array = []\n","    \n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","\n","    prev_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    with torch.no_grad():\n","        for batch_idx,(data, target) in enumerate(test_loader):\n","            \n","            hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","            data = data.type(torch.LongTensor).to(device)\n","            target = target.type(torch.LongTensor).to(device)\n","            hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","\n","            [hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed] = model(data, prev_hidden, cell_hidden, prev_hidden_words, cell_hidden_words,prev_hidden_reversed,cell_hidden_reversed,prev_hidden_words_reversed,cell_hidden_words_reversed)        \n","            logits = model.get_logits(hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed)          \n","            loss = model.get_loss(logits,target)\n","\n","\n","\n","            hidden_states.detach()\n","\n","            test_loss += loss.item()\n","            pred = logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            \n","            \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","            prediction_array.extend(pred.view(-1)[:].cpu().numpy())\n","            ground_truth_array.extend(target[:].cpu().numpy())\n","          \n","            \n","\n","   \n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    #plot_loss(test_loss,label='test',color='red')\n","    \n","    \n","    f1_scores = f1_score(ground_truth_array, prediction_array, average=None)\n","    current_accuracy = accuracy_score(ground_truth_array, prediction_array)\n","    \n","        \n","    print(classification_report(ground_truth_array, prediction_array, target_names=label_dict.getlabellist()))\n","    \n","    \n","    print('\\n')\n","    \n","    plot_confusion_matrix(ground_truth_array, prediction_array, classes=label_dict.getlabellist(),\n","                      title='Confusion matrix')\n","                                      \n","                                      \n","    return [f1_scores,current_accuracy,test_loss]\n","    \n","            "],"execution_count":0,"outputs":[]},{"metadata":{"id":"7I7k9ad5hii7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":726},"outputId":"82ac7e2b-17d1-4170-9e0c-31ca27ac34aa","executionInfo":{"status":"ok","timestamp":1555947998340,"user_tz":-180,"elapsed":6294,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}}},"cell_type":"code","source":["    drive.mount('/content/gdrive',force_remount=True)  \n","    %ls \"gdrive/My Drive/Datasets/Emoji/\"\n","    FunPath = \"gdrive/My Drive/Datasets/Emoji/newfun.txt\"\n","    [funwords,funlabels] = get_data(FunPath,label_dict)\n","    print(label_dict.getlabellist())\n","    data_loader_fun  = get_dataset_loader(funwords,funlabels,label_dict, vocab, wordvocab, _hyperparameters_dict['max_len'], _hyperparameters_dict['max_sentence_word_length'],_hyperparameters_dict['batch_size'])\n","  \n","  \n","  \n","  \n","    model=network\n","    test_loader = data_loader_fun\n","\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    \n","    ground_truth_array = []\n","    prediction_array = []\n","    \n","    prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","\n","    prev_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    prev_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    cell_hidden_words_reversed = torch.zeros(_hyperparameters_dict[\"batch_size\"],_hyperparameters_dict[\"rnn_size\"]).to(device)\n","    \n","    with torch.no_grad():\n","        for batch_idx,(data, target) in enumerate(test_loader):\n","            \n","            hidden_states_indexes = data[:,_hyperparameters_dict['max_len']:_hyperparameters_dict['max_len']+3]\n","            data = data.type(torch.LongTensor).to(device)\n","            target = target.type(torch.LongTensor).to(device)\n","            hidden_states_indexes = hidden_states_indexes.type(torch.LongTensor).to(device)\n","\n","            [hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed] = model(data, prev_hidden, cell_hidden, prev_hidden_words, cell_hidden_words,prev_hidden_reversed,cell_hidden_reversed,prev_hidden_words_reversed,cell_hidden_words_reversed)        \n","            logits = model.get_logits(hidden_states,hidden_states_words,hidden_states_reversed,hidden_states_words_reversed)          \n","            loss = model.get_loss(logits,target)\n","\n","\n","\n","            hidden_states.detach()\n","\n","            test_loss += loss.item()\n","            pred = logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            \n","            \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","            prediction_array.extend(pred.view(-1)[:].cpu().numpy())\n","            ground_truth_array.extend(target[:].cpu().numpy())\n","\n","    print(pred)\n","    print(label_dict.getlabellist())"],"execution_count":61,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","demojized_devEmotion.txt    demojized_trainEmotion.txt    newfile1.txt\n","demojized_dev_feelings.txt  demojized_train_feelings.txt  newfile.gdoc\n","demojized_dev.txt           demojized_train.txt           newfile.txt\n","demojized_test.txt          glove.twitter.27B.100d.txt    newfun.txt\n","gdrive/My Drive/Datasets/Emoji/newfun.txt\n","['others', 'angry', 'sad', 'happy']\n","tensor([[2],\n","        [2],\n","        [0],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [0],\n","        [1],\n","        [0],\n","        [1],\n","        [2],\n","        [1],\n","        [1],\n","        [0],\n","        [1],\n","        [0],\n","        [1],\n","        [2],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [0],\n","        [0],\n","        [2],\n","        [1]], device='cuda:0')\n","['others', 'angry', 'sad', 'happy']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"}]},{"metadata":{"id":"y9iV_tNzhuO0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"9BB0DbyrhuRA","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"1yDFjx6nhuTa","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pbo1bKIBhuV0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"kWgS-wJmhuYY","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"mBVKsNdRkxI0","colab_type":"code","outputId":"a6856287-d5cb-427e-8800-fe3fbf424f04","executionInfo":{"status":"ok","timestamp":1555945558710,"user_tz":-180,"elapsed":165531,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"cell_type":"code","source":["'''=================================  RUN THE TRAINING / TESTING  ================================='''\n","\n","network= RNNLM(vocab.size(),wordvocab.size(),_hyperparameters_dict['embedding_size'], _hyperparameters_dict['rnn_size'], _hyperparameters_dict['output_size'])\n","network = network.to(device)\n","\n","epochs = 2#hyperparameters_dict['num_epochs']\n","verbose = 100\n","\n","\n","optimizer = torch.optim.Adam(network.parameters(),lr=_hyperparameters_dict['learning_rate'])\n","\n","trainLosses = []\n","devLosses = []\n","testLosses = []\n","\n","trainF1 = []\n","devF1 = []\n","testF1 = []\n","\n","trainacc = []\n","devacc = []\n","testacc = []\n","\n","currentFall = 0\n","\n","print(label_dict.getlabellist())\n","for epoch in range(1, epochs):\n","    [f1train,accuracytrain,losstrain] = trainRNNM(network,device,data_loader_train,optimizer,epoch, verbose)\n","    \n","    trainLosses.append(losstrain)\n","    devLosses.append(lossdev)\n","    testLosses.append(losstest)\n","    \n","    trainF1.append(f1train)\n","    devF1.append(f1dev)\n","    testF1.append(f1test)\n","    \n","    \n","    trainacc.append(accuracytrain)\n","    devacc.append(accuracydev)\n","    testacc.append(accuracytest)\n","    \n","    \n","    if(epoch > 2):\n","      if devLosses[epoch-2] < devLosses[epoch-1]:\n","        currentFall = 1\n","      else:\n","        currentFall = 0\n","    \n","    if(currentFall == 3):\n","      break\n","\n","\n","plt.show()\n","\n","\n","plt.figure()      \n","      \n","plt.plot(trainLosses,color='b',label='train-loss')\n","plt.plot(devLosses,color='r',label='dev-loss')\n","plt.plot(testLosses,color='g',label='test-loss')\n","plt.ylabel('Mean Loss')\n","plt.xlabel('Epoch')\n","plt.title('Loss over time')\n","plt.legend()\n","\n","plt.show()\n","\n","\n","plt.figure()\n","plt.plot(trainacc,'b',label='train-acc')\n","plt.plot(devacc,'r',label='dev-acc')\n","plt.plot(testacc,'g',label='test-acc')\n","plt.ylabel('Accuracy Score')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","#plt.plot(trainLosses,'b')\n","#plt.plot(devLosses,'r')\n","#plt.plot(testLosses,'g')\n","#plt.show()\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["['others', 'angry', 'sad', 'happy']\n","Train Epoch: 1 [0/30160 (0%)]\tLoss: 1.384728\n","Train Epoch: 1 [3200/30160 (11%)]\tLoss: 1.220419\n","Train Epoch: 1 [6400/30160 (21%)]\tLoss: 1.342053\n","Train Epoch: 1 [9600/30160 (32%)]\tLoss: 1.110975\n","Train Epoch: 1 [12800/30160 (42%)]\tLoss: 1.121173\n","Train Epoch: 1 [16000/30160 (53%)]\tLoss: 0.931212\n","Train Epoch: 1 [19200/30160 (64%)]\tLoss: 0.886392\n","Train Epoch: 1 [22400/30160 (74%)]\tLoss: 0.968300\n","Train Epoch: 1 [25600/30160 (85%)]\tLoss: 0.790017\n","Train Epoch: 1 [28800/30160 (96%)]\tLoss: 0.534107\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMxScZCYmVkq/x/gKokOhZ0RXwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUv1/3Jkq8t2RQ/QAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"9dMeqAz9l_ak","colab_type":"code","outputId":"a7fcf4ab-ec40-4ff1-86a1-e306befdf8f3","executionInfo":{"status":"ok","timestamp":1555943670902,"user_tz":-180,"elapsed":793,"user":{"displayName":"Eduard","photoUrl":"https://lh3.googleusercontent.com/-IPPNQ13ivZQ/AAAAAAAAAAI/AAAAAAAABn0/iRrf1_Wbp_U/s64/photo.jpg","userId":"07524257311680339204"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"cell_type":"code","source":["print(devLosses)\n","print(trainLosses)\n","print(testLosses)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[0.014667656763495638, 0.013011211578729148, 0.011744784032368184, 0.009627105701315856]\n","[0.03185674399335125, 0.019228557381650496, 0.014554940228318346, 0.012212473054018751]\n","[0.014826005887673627, 0.013190703689348765, 0.011930518321602331, 0.009676440901084611]\n"],"name":"stdout"}]},{"metadata":{"id":"45NPYB6GFcyP","colab_type":"code","colab":{}},"cell_type":"code","source":["for batch_idx,(data,target) in enumerate(data_loader_train):\n","  print(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pAqB4F8eFmuo","colab_type":"code","colab":{}},"cell_type":"code","source":["a = torch.tensor([[1,2],[3,4]])\n","b = torch.tensor([[5,6],[10,11]])\n","print(a*b)\n","print(a-b)"],"execution_count":0,"outputs":[]}]}